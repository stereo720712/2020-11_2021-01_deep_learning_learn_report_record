<!DOCTYPE html>
<html data-theme="light" data-react-helmet="data-theme" lang="zh"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Batch Normalization原理与实战 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="keywords" content="深度学习（Deep Learning）,机器学习,TensorFlow 学习"><meta data-react-helmet="true" name="description" content="好久没有更新专栏了，从去年6月开始一直在忙实习，年初实习结束了又在写毕业论文，终于搞的差不多了，可以抽空来慢慢更新专栏内容了！ 前言本期专栏主要来从理论与实战视角对深度学习中的Batch Normalization的思…"><meta data-react-helmet="true" property="og:title" content="Batch Normalization原理与实战"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/34879333"><meta data-react-helmet="true" property="og:description" content="好久没有更新专栏了，从去年6月开始一直在忙实习，年初实习结束了又在写毕业论文，终于搞的差不多了，可以抽空来慢慢更新专栏内容了！ 前言本期专栏主要来从理论与实战视角对深度学习中的Batch Normalization的思…"><meta data-react-helmet="true" property="og:image" content="https://pic4.zhimg.com/v2-b3126b47b329a0141944b11ede097a8f_720w.jpg?source=172ae18b"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_006.css" rel="stylesheet"><script defer="defer" crossorigin="anonymous" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;987-95d97393&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can't find variable: webkit&quot;,&quot;Can't find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><script charset="utf-8" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_004.js"></script><link rel="stylesheet" type="text/css" href="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column.css"><script charset="utf-8" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_005.js"></script><link rel="stylesheet" type="text/css" href="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_005.css"><script charset="utf-8" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_007.js"></script><link rel="stylesheet" type="text/css" href="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_003.css"><script charset="utf-8" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column.js"></script><link rel="stylesheet" type="text/css" href="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_002.css"><script charset="utf-8" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_002.js"></script><style data-emotion="css"></style><link rel="stylesheet" type="text/css" href="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_004.css"><script charset="utf-8" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_003.js"></script></head><body class="WhiteBg-body" data-react-helmet="class"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;天雨粟&quot;,&quot;itemId&quot;:34879333,&quot;title&quot;:&quot;Batch Normalization原理与实战&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;34879333&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1339px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a class="ZhihuLogoLink" href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://www.zhihu.com/column/zhaoyeyu">机器不学习</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div><div class="Sticky--holder" style="position: relative; inset: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><img class="TitleImage" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-b3126b47b329a0141944b11ede097a8f_1440w.jpg" alt="Batch Normalization原理与实战"><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">Batch Normalization原理与实战</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="天雨粟"><meta itemprop="image" content="https://pic4.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b"><meta itemprop="url" content="https://www.zhihu.com/people/zhao-xie-yu-30"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30"><img class="Avatar Avatar--round AuthorInfo-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-61901a9da09b1ea674e50b98a4279202_xs.jpg" srcset="https://pic4.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b 2x" alt="天雨粟" width="38" height="38"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">计算广告/CTR/算法/工程</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">1,711 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>好久没有更新专栏了，从去年6月开始一直在忙实习，年初实习结束了又在写毕业论文，终于搞的差不多了，可以抽空来慢慢更新专栏内容了！</p><h2>前言</h2><p>本期专栏主要来从理论与实战视角对深度学习中的Batch Normalization的思路进行讲解、归纳和总结，并辅以代码让小伙伴儿们对Batch Normalization的作用有更加直观的了解。</p><p>本文主要分为两大部分。<b>第一部分是理论板块</b>，主要从背景、算法、效果等角度对Batch Normalization进行详解；<b>第二部分是实战板块</b>，主要以MNIST数据集作为整个代码测试的数据，通过比较加入Batch Normalization前后网络的性能来让大家对Batch Normalization的作用与效果有更加直观的感知。</p><hr><h2>（一）理论板块</h2><p>理论板块将从以下四个方面对Batch Normalization进行详解：</p><ul><li>提出背景</li><li>BN算法思想</li><li>测试阶段如何使用BN</li><li>BN的优势</li></ul><p>理论部分主要参考2015年Google的Sergey Ioffe与Christian Szegedy的论文内容，并辅以吴恩达Coursera课程与其它博主的资料。所有参考内容链接均见于文章最后参考链接部分。</p><h2>1 提出背景</h2><h2>1.1 炼丹的困扰</h2><p>在
深度学习中，由于问题的复杂性，我们往往会使用较深层数的网络进行训练，相信很多炼丹的朋友都对调参的困难有所体会，尤其是对深层神经网络的训练调参更是
困难且复杂。在这个过程中，我们需要去尝试不同的学习率、初始化参数方法（例如Xavier初始化）等方式来帮助我们的模型加速收敛。深度神经网络之所以
如此难训练，其中一个重要原因就是网络中层与层之间存在高度的关联性与耦合性。下图是一个多层的神经网络，层与层之间采用全连接的方式进行连接。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1694" data-rawheight="1052" class="origin_image zh-lightbox-thumb" width="1694" data-original="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_r.jpg"/></noscript><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-c3934c77a543b6c588af07a365f9a70b_720w.jpg" data-caption="" data-size="normal" data-rawwidth="1694" data-rawheight="1052" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_b.jpg" data-lazy-status="ok" width="1694"></figure><p class="ztext-empty-paragraph"><br></p><p>我
们规定左侧为神经网络的底层，右侧为神经网络的上层。那么网络中层与层之间的关联性会导致如下的状况：随着训练的进行，网络中的参数也随着梯度下降在不停
更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另
一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做
Internal Covariate Shift。</p><h2>1.2 什么是Internal Covariate Shift</h2><p>Batch
 Normalization的原论文作者给了Internal Covariate 
Shift一个较规范的定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal 
Covariate Shift。</p><p>这句话该怎么理解呢？我们同样以1.1中的图为例，我们定义每一层的线性变换为 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_065.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}=W^{[l]}\times input+b^{[l]}">，其中 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_013.svg" alt="[公式]" eeimg="1" data-formula="l "> 代表层数；非线性变换为 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_044.svg" alt="[公式]" eeimg="1" data-formula="A^{[l]}=g^{[l]}(Z^{[l]})"> ，其中 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_045.svg" alt="[公式]" eeimg="1" data-formula="g^{[l]}(\cdot)"> 为第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的激活函数。</p><p>随着梯度下降的进行，每一层的参数 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_009.svg" alt="[公式]" eeimg="1" data-formula="W^{[l]}"> 与 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_012.svg" alt="[公式]" eeimg="1" data-formula="b^{[l]}"> 都会被更新，那么 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_055.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}"> 的分布也就发生了改变，进而 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_010.svg" alt="[公式]" eeimg="1" data-formula="A^{[l]}"> 也同样出现分布的改变。而 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_010.svg" alt="[公式]" eeimg="1" data-formula="A^{[l]}"> 作为第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_046.svg" alt="[公式]" eeimg="1" data-formula="l+1"> 层的输入，意味着 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_046.svg" alt="[公式]" eeimg="1" data-formula="l+1"> 层就需要去不停适应这种数据分布的变化，这一过程就被叫做Internal Covariate Shift。</p><h2>1.3 Internal Covariate Shift会带来什么问题？</h2><p><b>（1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</b></p><p>我们在上面提到了梯度下降的过程会让每一层的参数 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_009.svg" alt="[公式]" eeimg="1" data-formula="W^{[l]}"> 和 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_012.svg" alt="[公式]" eeimg="1" data-formula="b^{[l]}"> 发生变化，进而使得每一层的线性与非线性计算结果分布产生变化。后层网络就要不停地去适应这种分布变化，这个时候就会使得整个网络的学习速率过慢。</p><p><b>（2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度</b></p><p>当我们在神经网络中采用饱和激活函数（saturated activation function）时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区（saturated regime）。随着模型训练的进行，我们的参数 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_009.svg" alt="[公式]" eeimg="1" data-formula="W^{[l]}"> 会逐渐更新并变大，此时 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_069.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}"> 就会随之变大，并且 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_055.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}"> 还受到更底层网络参数 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_004.svg" alt="[公式]" eeimg="1" data-formula="W^{[1]},W^{[2]},\cdots,W^{[l-1]}"> 的影响，随着网络层数的加深， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_055.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}"> 很容易陷入梯度饱和区，此时梯度会变得很小甚至接近于0，参数的更新速度就会减慢，进而就会放慢网络的收敛速度。</p><p>对
于激活函数梯度饱和问题，有两种解决思路。第一种就是更为非饱和性激活函数，例如线性整流函数ReLU可以在一定程度上解决训练进入梯度饱和区的问题。另
一种思路是，我们可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。</p><h2>1.4 我们如何减缓Internal Covariate Shift？</h2><p>要缓解ICS的问题，就要明白它产生的原因。ICS产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可以通过固定每一层网络输入值的分布来对减缓ICS问题。</p><p><b>（1）白化（Whitening）</b></p><p>白化（Whitening）是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：</p><ul><li><b>使得输入特征分布具有相同的均值与方差。</b>其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；</li><li><b>去除特征之间的相关性。</b></li></ul><p>通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛（LeCun et al.,1998b；Wiesler&amp;Ney,2011）。</p><p><b>（2）Batch Normalization提出</b></p><p>既然白化可以解决这个问题，为什么我们还要提出别的解决办法？当然是现有的方法具有一定的缺陷，白化主要有以下两个问题：</p><ul><li><b>白化过程计算成本太高，</b>并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作；</li><li><b>白化过程由于改变了网络每一层的分布</b>，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。</li></ul><p>既然有了上面两个问题，那我们的解决思路就很简单，一方面，我们提出的normalization方法要能够简化计算过程；另一方面又需要经过规范化处理后让数据尽可能保留原始的表达能力。于是就有了简化+改进版的白化——Batch Normalization。</p><h2>2 Batch Normalization</h2><h2>2.1 思路</h2><p>既然白化计算过程比较复杂，那我们就简化一点，比如我们可以尝试单独对每个特征进行normalizaiton就可以了，让每个特征都有均值为0，方差为1的分布就OK。</p><p>另一个问题，既然白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。</p><p>因此，基于上面两个解决问题的思路，作者提出了Batch Normalization，下一部分来具体讲解这个算法步骤。</p><h2>2.2 算法</h2><p>在深度学习中，由于采用full batch的训练方式对内存要求较大，且每一轮训练时间过长；我们一般都会采用对数据做划分，用mini-batch对网络进行训练。因此，Batch Normalization也就在mini-batch的基础上进行计算。</p><h2>2.2.1 参数定义</h2><p>我们依旧以下图这个神经网络为例。我们定义网络总共有 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_060.svg" alt="[公式]" eeimg="1" data-formula="L"> 层（不包含输入层）并定义如下符号：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1694" data-rawheight="1052" class="origin_image zh-lightbox-thumb" width="1694" data-original="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_r.jpg"/></noscript><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-c3934c77a543b6c588af07a365f9a70b_720w.jpg" data-caption="" data-size="normal" data-rawwidth="1694" data-rawheight="1052" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c3934c77a543b6c588af07a365f9a70b_b.jpg" data-lazy-status="ok" width="1694"></figure><p><b>参数相关：</b></p><ul><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> ：网络中的层标号</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_060.svg" alt="[公式]" eeimg="1" data-formula="L"> ：网络中的最后一层或总层数</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_062.svg" alt="[公式]" eeimg="1" data-formula="d_l"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的维度，即神经元结点数</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_009.svg" alt="[公式]" eeimg="1" data-formula="W^{[l]}"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的权重矩阵， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_061.svg" alt="[公式]" eeimg="1" data-formula="W^{[l]}\in \mathbb{R}^{d_l\times d_{l-1}}"> </li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_012.svg" alt="[公式]" eeimg="1" data-formula="b^{[l]}"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的偏置向量， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_011.svg" alt="[公式]" eeimg="1" data-formula="b^{[l]}\in \mathbb{R}^{d_l\times 1}"> </li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_055.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的线性计算结果， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_065.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}=W^{[l]}\times input+b^{[l]}"> </li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_045.svg" alt="[公式]" eeimg="1" data-formula="g^{[l]}(\cdot)"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的激活函数</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_010.svg" alt="[公式]" eeimg="1" data-formula="A^{[l]}"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的非线性激活结果， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_044.svg" alt="[公式]" eeimg="1" data-formula="A^{[l]}=g^{[l]}(Z^{[l]})"> </li></ul><p><b>样本相关：</b></p><ul><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_048.svg" alt="[公式]" eeimg="1" data-formula="M"> ：训练样本的数量</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation.svg" alt="[公式]" eeimg="1" data-formula="N"> ：训练样本的特征数</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_037.svg" alt="[公式]" eeimg="1" data-formula="X"> ：训练样本集， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_021.svg" alt="[公式]" eeimg="1" data-formula="X=\{x^{(1)},x^{(2)},\cdots,x^{(M)}\}，X\in \mathbb{R}^{N\times M}"> （注意这里 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_037.svg" alt="[公式]" eeimg="1" data-formula="X"> 的一列是一个样本）</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_018.svg" alt="[公式]" eeimg="1" data-formula="m"> ：batch size，即每个batch中样本的数量</li><li><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_008.svg" alt="[公式]" eeimg="1" data-formula="\chi^{(i)}"> ：第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_023.svg" alt="[公式]" eeimg="1" data-formula="i"> 个mini-batch的训练数据， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_030.svg" alt="[公式]" eeimg="1" data-formula="X= \{\chi^{(1)},\chi^{(2)},\cdots,\chi^{(k)}\}"> ，其中 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_017.svg" alt="[公式]" eeimg="1" data-formula="\chi^{(i)}\in \mathbb{R}^{N\times m}"> </li></ul><h2>2.2.2 算法步骤</h2><p>介绍算法思路沿袭前面BN提出的思路来讲。第一点，对每个特征进行独立的normalization。我们考虑一个batch的训练，传入m个训练样本，并关注网络中的某一层，忽略上标 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 。</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_052.svg" alt="[公式]" eeimg="1" data-formula="Z\in \mathbb{R}^{d_l\times m}"> </p><p>我们关注当前层的第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_049.svg" alt="[公式]" eeimg="1" data-formula="j"> 个维度，也就是第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_049.svg" alt="[公式]" eeimg="1" data-formula="j"> 个神经元结点，则有 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_036.svg" alt="[公式]" eeimg="1" data-formula="Z_j\in \mathbb{R}^{1\times m}"> 。我们当前维度进行规范化：</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_056.svg" alt="[公式]" eeimg="1" data-formula="\mu_j=\frac{1}{m}\sum_{i=1}^m Z_j^{(i)}"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_067.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_j=\frac{1}{m}\sum_{i=1}^m(Z_j^{(i)}-\mu_j)^2"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_050.svg" alt="[公式]" eeimg="1" data-formula="\hat{Z}_j=\frac{Z_j-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}"> </p><blockquote>其中 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_047.svg" alt="[公式]" eeimg="1" data-formula="\epsilon"> 是为了防止方差为0产生无效计算。</blockquote><p>下面我们再来结合个具体的例子来进行计算。下图我们只关注第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的计算结果，左边的矩阵是 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_069.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}"> 线性计算结果，还未进行激活函数的非线性变换。此时每一列是一个样本，图中可以看到共有8列，代表当前训练样本的batch中共有8个样本，每一行代表当前 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层神经元的一个节点，可以看到当前 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层共有4个神经元结点，即第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层维度为4。我们可以看到，每行的数据分布都不同。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-084e9875d10896369e09af5a60e56250_b.jpg" data-caption="" data-size="normal" data-rawwidth="2176" data-rawheight="1140" class="origin_image zh-lightbox-thumb" width="2176" data-original="https://pic1.zhimg.com/v2-084e9875d10896369e09af5a60e56250_r.jpg"/></noscript><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-084e9875d10896369e09af5a60e56250_720w.jpg" data-caption="" data-size="normal" data-rawwidth="2176" data-rawheight="1140" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic1.zhimg.com/v2-084e9875d10896369e09af5a60e56250_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-084e9875d10896369e09af5a60e56250_b.jpg" data-lazy-status="ok" width="2176"></figure><p>对于第一个神经元，我们求得 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_024.svg" alt="[公式]" eeimg="1" data-formula="\mu_1=1.65"> ， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_016.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_1=0.44"> （其中 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_057.svg" alt="[公式]" eeimg="1" data-formula="\epsilon=10^{-8}"> ），此时我们利用 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_053.svg" alt="[公式]" eeimg="1" data-formula="\mu_1,\sigma^2_1"> 对第一行数据（第一个维度）进行normalization得到新的值 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_006.svg" alt="[公式]" eeimg="1" data-formula="[-0.98,-0.23,-0.68,-1.13,0.08,0.68,2.19,0.08]"> 。同理我们可以计算出其他输入维度归一化后的值。如下图：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-c37bda8f138402cc7c3dd62c509d36f6_b.jpg" data-caption="" data-size="normal" data-rawwidth="2266" data-rawheight="1142" class="origin_image zh-lightbox-thumb" width="2266" data-original="https://pic3.zhimg.com/v2-c37bda8f138402cc7c3dd62c509d36f6_r.jpg"/></noscript><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-c37bda8f138402cc7c3dd62c509d36f6_720w.jpg" data-caption="" data-size="normal" data-rawwidth="2266" data-rawheight="1142" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic3.zhimg.com/v2-c37bda8f138402cc7c3dd62c509d36f6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-c37bda8f138402cc7c3dd62c509d36f6_b.jpg" data-lazy-status="ok" width="2266"></figure><p>通过上面的变换，<b>我们解决了第一个问题，即用更加简化的方式来对数据进行规范化，使得第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层的输入每个特征的分布均值为0，方差为1。</b></p><p>如
同上面提到的，Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是
我们通过变换操作改变了原有数据的信息表达（representation ability of the 
network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh
激活函数时，容易陷入非线性激活函数的线性区域。<br></p><p>因此，BN又引入了两个可学习（learnable）的参数 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_063.svg" alt="[公式]" eeimg="1" data-formula="\gamma"> 与 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_032.svg" alt="[公式]" eeimg="1" data-formula="\beta"> 。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_070.svg" alt="[公式]" eeimg="1" data-formula="\tilde{Z_j}=\gamma_j \hat{Z}_j+\beta_j"> 。特别地，当 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_035.svg" alt="[公式]" eeimg="1" data-formula="\gamma^2=\sigma^2,\beta=\mu"> 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。</p><p><b>通过上面的步骤，我们就在一定程度上保证了输入数据的表达能力。</b></p><p>以上就是整个Batch Normalization在模型训练中的算法和思路。</p><blockquote>补充： 在进行normalization的过程中，由于我们的规范化操作会对减去均值，因此，偏置项 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_041.svg" alt="[公式]" eeimg="1" data-formula="b"> 可以被忽略掉或可以被置为0，即 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_020.svg" alt="[公式]" eeimg="1" data-formula="BN(Wu+b)=BN(Wu)"> </blockquote><h2>2.2.3 公式</h2><p>对于神经网络中的第 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_028.svg" alt="[公式]" eeimg="1" data-formula="l"> 层，我们有：</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_069.svg" alt="[公式]" eeimg="1" data-formula="Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_064.svg" alt="[公式]" eeimg="1" data-formula="\mu=\frac{1}{m}\sum_{i=1}^mZ^{[l](i)}"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_007.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2=\frac{1}{m}\sum_{i=1}^m(Z^{[l](i)}-\mu)^2"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_058.svg" alt="[公式]" eeimg="1" data-formula="\tilde{Z}^{[l]}=\gamma\cdot\frac{Z^{[l]}-\mu}{\sqrt{\sigma^2+\epsilon}}+\beta"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_042.svg" alt="[公式]" eeimg="1" data-formula="A^{[l]}=g^{[l]}(\tilde{Z}^{[l]})"> </p><p class="ztext-empty-paragraph"><br></p><h2>3 测试阶段如何使用Batch Normalization？</h2><p>我们知道BN在每一层计算的 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_059.svg" alt="[公式]" eeimg="1" data-formula="\mu"> 与 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_043.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2"> 都是基于当前batch中的训练数据，但是这就带来了一个问题：我们在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_059.svg" alt="[公式]" eeimg="1" data-formula="\mu"> 与 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_043.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2"> 的计算一定是有偏估计，这个时候我们该如何进行计算呢？</p><p>利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_054.svg" alt="[公式]" eeimg="1" data-formula="\mu_{batch}"> 与 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_040.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_{batch}"> 。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_015.svg" alt="[公式]" eeimg="1" data-formula="\mu_{test}=\mathbb{E} (\mu_{batch})"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_066.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_{test}=\frac{m}{m-1}\mathbb{E}(\sigma^2_{batch})"> </p><p>得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_003.svg" alt="[公式]" eeimg="1" data-formula="BN(X_{test})=\gamma\cdot \frac{X_{test}-\mu_{test}}{\sqrt{\sigma^2_{test}+\epsilon}}+\beta"> </p><p>另外，除了采用整体样本的无偏估计外。吴恩达在Coursera上的Deep Learning课程指出可以对train阶段每个batch计算的mean/variance采用指数加权平均来得到test阶段mean/variance的估计。</p><h2>4 Batch Normalization的优势</h2><p>Batch Normalization在实际工程中被证明了能够缓解神经网络难以训练的问题，BN具有的有事可以总结为以下三点：</p><p><b>（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</b></p><p>BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。</p><p><b>（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</b></p><p>在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。</p><p>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。例如，我们对参数 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_038.svg" alt="[公式]" eeimg="1" data-formula="W"> 进行缩放得到 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_019.svg" alt="[公式]" eeimg="1" data-formula="aW"> 。对于缩放前的值 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_005.svg" alt="[公式]" eeimg="1" data-formula="Wu"> ，我们设其均值为 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_033.svg" alt="[公式]" eeimg="1" data-formula="\mu_1"> ，方差为 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_034.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_1"> ；对于缩放值 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_022.svg" alt="[公式]" eeimg="1" data-formula="aWu"> ，设其均值为 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_039.svg" alt="[公式]" eeimg="1" data-formula="\mu_2"> ，方差为 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_026.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_2"> ，则我们有：</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_031.svg" alt="[公式]" eeimg="1" data-formula="\mu_2=a\mu_1"> ， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_014.svg" alt="[公式]" eeimg="1" data-formula="\sigma^2_2=a^2\sigma^2_1"> </p><p>我们忽略 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_047.svg" alt="[公式]" eeimg="1" data-formula="\epsilon"> ，则有：</p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_002.svg" alt="[公式]" eeimg="1" data-formula="BN(aWu)=\gamma\cdot\frac{aWu-\mu_2}{\sqrt{\sigma^2_2}}+\beta=\gamma\cdot\frac{aWu-a\mu_1}{\sqrt{a^2\sigma^2_1}}+\beta=\gamma\cdot\frac{Wu-\mu_1}{\sqrt{\sigma^2_1}}+\beta=BN(Wu)"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_068.svg" alt="[公式]" eeimg="1" data-formula="\frac{\partial{BN((aW)u)}}{\partial{u}}=\gamma\cdot\frac{aW}{\sqrt{\sigma^2_2}}=\gamma\cdot\frac{aW}{\sqrt{a^2\sigma^2_1}}=\frac{\partial{BN(Wu)}}{\partial{u}}"> </p><p><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_025.svg" alt="[公式]" eeimg="1" data-formula="\frac{\partial{BN((aW)u)}}{\partial{(aW)}}=\gamma\cdot\frac{u}{\sqrt{\sigma^2_2}}=\gamma\cdot\frac{u}{a\sqrt{\sigma^2_1}}=\frac{1}{a}\cdot\frac{\partial{BN(Wu)}}{\partial{W}}"> </p><blockquote>注：公式中的 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_027.svg" alt="[公式]" eeimg="1" data-formula="u"> 是当前层的输入，也是前一层的输出；不是下标啊旁友们！</blockquote><p>我们可以看到，经过BN操作以后，权重的缩放值会被“抹去”，因此保证了输入数据分布稳定在一定范围内。另外，权重的缩放并不会影响到对 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_027.svg" alt="[公式]" eeimg="1" data-formula="u"> 的梯度计算；并且当权重越大时，即 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_051.svg" alt="[公式]" eeimg="1" data-formula="a"> 越大， <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_029.svg" alt="[公式]" eeimg="1" data-formula="\frac{1}{a}"> 越小，意味着权重 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_038.svg" alt="[公式]" eeimg="1" data-formula="W"> 的梯度反而越小，这样BN就保证了梯度不会依赖于参数的scale，使得参数的更新处在更加稳定的状态。</p><p>因此，在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。</p><p><b>（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</b></p><p>在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_063.svg" alt="[公式]" eeimg="1" data-formula="\gamma"> 与 <img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/equation_032.svg" alt="[公式]" eeimg="1" data-formula="\beta"> 又让数据保留更多的原始信息。</p><p><b>（4）BN具有一定的正则化效果</b></p><p>在Batch
 
Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从
总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络
训练带来噪音类似，在一定程度上对模型起到了正则化的效果。</p><p>另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。</p><hr><h2>（二）实战板块</h2><p>经
过了上面了理论学习，我们对BN有了理论上的认知。“Talk is cheap, show me the 
code”。接下来我们就通过实际的代码来对比加入BN前后的模型效果。实战部分使用MNIST数据集作为数据基础，并使用TensorFlow中的
Batch Normalization结构来进行BN的实现。</p><p>数据准备：MNIST手写数据集</p><p>代码地址：我的<a href="https://link.zhihu.com/?target=https%3A//github.com/NELSONZHAO/zhihu/tree/master/batch_normalization_discussion" class=" wrap external" target="_blank" rel="nofollow noreferrer">GitHub</a></p><p><b>注：TensorFlow版本为1.6.0</b></p><p>实战板块主要分为两部分：</p><ul><li>网络构建与辅助函数</li><li>BN测试</li></ul><h2>1 网络构建与辅助函数</h2><p>首先我们先定义一下神经网络的类，这个类里面主要包括了以下方法：</p><ul><li>build_network：前向计算</li><li>fully_connected：全连接计算</li><li>train：训练模型</li><li>test：测试模型</li></ul><h2>1.1 build_network</h2><p>我
们首先通过构造函数，把权重、激活函数以及是否使用BN这些变量传入，并生成一个training_accuracies来记录训练过程中的模型准确率变
化。这里的initial_weights是一个list，list中每一个元素是一个矩阵（二维tuple），存储了每一层的权重矩阵。
build_network实现了网络的构建，并调用了fully_connected函数（下面会提）进行计算。要注意的是，由于MNIST是多分类，
在这里我们不需要对最后一层进行激活，保留计算的logits就好。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-11f333fecc38621169bfa37d52e35bc2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1760" data-rawheight="1226" class="origin_image zh-lightbox-thumb" width="1760" data-original="https://pic3.zhimg.com/v2-11f333fecc38621169bfa37d52e35bc2_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1760' height='1226'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1760" data-rawheight="1226" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic3.zhimg.com/v2-11f333fecc38621169bfa37d52e35bc2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-11f333fecc38621169bfa37d52e35bc2_b.jpg" width="1760"></figure><h2>1.2 fully_connected</h2><p>这里的fully_connected主要用来每一层的线性与非线性计算。通过self.use_batch_norm来控制是否使用BN。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-cbea7e029dc0820d0ff69bbbef9fc995_b.jpg" data-caption="" data-size="normal" data-rawwidth="1992" data-rawheight="748" class="origin_image zh-lightbox-thumb" width="1992" data-original="https://pic2.zhimg.com/v2-cbea7e029dc0820d0ff69bbbef9fc995_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1992' height='748'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1992" data-rawheight="748" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic2.zhimg.com/v2-cbea7e029dc0820d0ff69bbbef9fc995_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-cbea7e029dc0820d0ff69bbbef9fc995_b.jpg" width="1992"></figure><p>另外，值得注意的是，tf.layers.batch_normalization接口中training参数非常重要，官方文档中描述为：</p><blockquote><b><code>training</code></b>:
 Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a 
placeholder). Whether to return the output in training mode (normalized 
with statistics of the current batch) or in inference mode (normalized 
with moving statistics). <b>NOTE</b>: make sure to set this parameter correctly, or else your training/inference will not work properly.</blockquote><p>当我们训练时，要设置为True，保证在训练过程中使用的是mini-batch的统计量进行normalization；在Inference阶段，使用False，也就是使用总体样本的无偏估计。</p><h2>1.3 train</h2><p>train函数主要用来进行模型的训练。除了要定义label，loss以及optimizer以外，我们还需要注意，官方文档指出在使用BN时的事项：</p><blockquote><b>Note:</b> when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in <code>tf.GraphKeys.UPDATE_OPS</code>, so they need to be added as a dependency to the <code>train_op</code>. </blockquote><p>因此当self.use_batch_norm为True时，要使用tf.control_dependencies保证模型正常训练。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-dd2cfbaadd03f1f35b9f77a5ac570d75_b.jpg" data-caption="" data-size="normal" data-rawwidth="1988" data-rawheight="1636" class="origin_image zh-lightbox-thumb" width="1988" data-original="https://pic2.zhimg.com/v2-dd2cfbaadd03f1f35b9f77a5ac570d75_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1988' height='1636'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1988" data-rawheight="1636" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic2.zhimg.com/v2-dd2cfbaadd03f1f35b9f77a5ac570d75_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-dd2cfbaadd03f1f35b9f77a5ac570d75_b.jpg" width="1988"></figure><blockquote>注
意：在训练过程中batch_size选了60（mnist.train.next_batch(60)），这里是因为BN的原paper中用的60。(
 We trained the network for 50000 steps, with 60 examples per 
mini-batch.)</blockquote><h2>1.4 test</h2><p>test阶段与train类似，只是要设置self.is_training=False，保证Inference阶段BN的正确。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-8f42ba5dc8755655bc1d5ad36ae6d0e1_b.jpg" data-caption="" data-size="normal" data-rawwidth="1668" data-rawheight="672" class="origin_image zh-lightbox-thumb" width="1668" data-original="https://pic2.zhimg.com/v2-8f42ba5dc8755655bc1d5ad36ae6d0e1_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1668' height='672'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1668" data-rawheight="672" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic2.zhimg.com/v2-8f42ba5dc8755655bc1d5ad36ae6d0e1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-8f42ba5dc8755655bc1d5ad36ae6d0e1_b.jpg" width="1668"></figure><p class="ztext-empty-paragraph"><br></p><p>经过上面的步骤，我们的框架基本就搭好了，接下来我们再写一个辅助函数train_and_test以及plot绘图函数就可以开始对BN进行测试啦。train_and_test以及plot函数见GitHub代码中，这里不再赘述。</p><h2>2 BN测试</h2><p>在这里，我们构造一个4层神经网络，输入层结点数784，三个隐层均为128维，输出层10个结点，如下图所示：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-a9555e40a7395054b24e0c8a2d89a578_b.jpg" data-caption="" data-size="normal" data-rawwidth="1362" data-rawheight="1344" class="origin_image zh-lightbox-thumb" width="1362" data-original="https://pic1.zhimg.com/v2-a9555e40a7395054b24e0c8a2d89a578_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1362' height='1344'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1362" data-rawheight="1344" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic1.zhimg.com/v2-a9555e40a7395054b24e0c8a2d89a578_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a9555e40a7395054b24e0c8a2d89a578_b.jpg" width="1362"></figure><p>实验中，我们主要控制一下三个变量：</p><ul><li>权重矩阵（较小初始化权重，标准差为0.05；较大初始化权重，标准差为10）</li><li>学习率（较小学习率：0.01；较大学习率：2）</li><li>隐层激活函数（relu，sigmoid）</li></ul><h2>2.1 小权重，小学习率，ReLU</h2><p>测试结果如下图：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-1db5b7e223ef0e4688e233f3e3f30c51_b.jpg" data-caption="" data-size="normal" data-rawwidth="1546" data-rawheight="1156" class="origin_image zh-lightbox-thumb" width="1546" data-original="https://pic2.zhimg.com/v2-1db5b7e223ef0e4688e233f3e3f30c51_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1546' height='1156'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1546" data-rawheight="1156" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic2.zhimg.com/v2-1db5b7e223ef0e4688e233f3e3f30c51_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-1db5b7e223ef0e4688e233f3e3f30c51_b.jpg" width="1546"></figure><p>我们可以得到以下结论：</p><ul><li>在训练与预测阶段，加入BN的模型准确率都稍高一点；</li><li>加入BN的网络收敛更快（黄线）</li><li>没有加入BN的网络训练速度更快（483.61it/s&gt;329.23it/s），这是因为BN增加了神经网络中的计算量</li></ul><p>为了更清楚地看到BN收敛速度更快，我们把减少Training batches，设置为3000，得到如下结果：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-04ed801797f69120fc601933b48aee28_b.jpg" data-caption="" data-size="normal" data-rawwidth="1526" data-rawheight="1164" class="origin_image zh-lightbox-thumb" width="1526" data-original="https://pic1.zhimg.com/v2-04ed801797f69120fc601933b48aee28_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1526' height='1164'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1526" data-rawheight="1164" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic1.zhimg.com/v2-04ed801797f69120fc601933b48aee28_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-04ed801797f69120fc601933b48aee28_b.jpg" width="1526"></figure><p>从上图中我们就可以清晰看到，加入BN的网络在第500个batch的时候已经能够在validation数据集上达到90%的准确率；而没有BN的网络的准确率还在不停波动，并且到第3000个batch的时候才达到90%的准确率。</p><h2>2.2 小权重，小学习率，Sigmoid</h2><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-dd09009ee3937fa0a4268b7b0eb22397_b.jpg" data-caption="" data-size="normal" data-rawwidth="1602" data-rawheight="1156" class="origin_image zh-lightbox-thumb" width="1602" data-original="https://pic4.zhimg.com/v2-dd09009ee3937fa0a4268b7b0eb22397_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1602' height='1156'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1602" data-rawheight="1156" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-dd09009ee3937fa0a4268b7b0eb22397_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-dd09009ee3937fa0a4268b7b0eb22397_b.jpg" width="1602"></figure><p>学习率与权重均没变，我们把隐层激活函数换为sigmoid。可以发现，BN收敛速度非常之快，而没有BN的网络前期在不断波动，直到第20000个train batch以后才开始进入平稳的训练状态。</p><h2>2.3 小权重，大学习率，ReLU</h2><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-899b1d8fb2c372a7a6829adc10aab0f0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1558" data-rawheight="1142" class="origin_image zh-lightbox-thumb" width="1558" data-original="https://pic1.zhimg.com/v2-899b1d8fb2c372a7a6829adc10aab0f0_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1558' height='1142'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1558" data-rawheight="1142" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic1.zhimg.com/v2-899b1d8fb2c372a7a6829adc10aab0f0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-899b1d8fb2c372a7a6829adc10aab0f0_b.jpg" width="1558"></figure><p>在本次实验中，我们使用了较大的学习率，较大的学习率意味着权重的更新跨度很大，而根据我们前面理论部分的介绍，BN不会受到权重scale的影响，因此其能够使模型保持在一个稳定的训练状态；而没有加入BN的网络则在一开始就由于学习率过大导致训练失败。</p><h2>2.4 小权重，大学习率，Sigmoid</h2><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-50bd080b121e64376c2e818f9651bdec_b.jpg" data-caption="" data-size="normal" data-rawwidth="1512" data-rawheight="1144" class="origin_image zh-lightbox-thumb" width="1512" data-original="https://pic1.zhimg.com/v2-50bd080b121e64376c2e818f9651bdec_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1512' height='1144'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1512" data-rawheight="1144" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic1.zhimg.com/v2-50bd080b121e64376c2e818f9651bdec_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-50bd080b121e64376c2e818f9651bdec_b.jpg" width="1512"></figure><p>在保持较大学习率（learning rate=2）的情况下，当我们将激活函数换为sigmoid以后，两个模型都能够达到一个很好的效果，并且在test数据及上的准确率非常接近；但加入BN的网络要收敛地更快，同样的，我们来观察3000次batch的训练准确率。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-40dec0842bb3f78024514ae0fae2c3cf_b.jpg" data-caption="" data-size="normal" data-rawwidth="1556" data-rawheight="1156" class="origin_image zh-lightbox-thumb" width="1556" data-original="https://pic4.zhimg.com/v2-40dec0842bb3f78024514ae0fae2c3cf_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1556' height='1156'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1556" data-rawheight="1156" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-40dec0842bb3f78024514ae0fae2c3cf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-40dec0842bb3f78024514ae0fae2c3cf_b.jpg" width="1556"></figure><p>当
我们把training 
batch限制到3000以后，可以发现加入BN后，尽管我们使用较大的学习率，其仍然能够在大约500个batch以后在validation上达到
90%的准确率；但不加入BN的准确率前期在一直大幅度波动，到大约1000个batch以后才达到90%的准确率。</p><h2>2.5 大权重，小学习率，ReLU</h2><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-c17a9620dc1f6ee776fa398e3c73c453_b.jpg" data-caption="" data-size="normal" data-rawwidth="1494" data-rawheight="1144" class="origin_image zh-lightbox-thumb" width="1494" data-original="https://pic4.zhimg.com/v2-c17a9620dc1f6ee776fa398e3c73c453_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1494' height='1144'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1494" data-rawheight="1144" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-c17a9620dc1f6ee776fa398e3c73c453_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c17a9620dc1f6ee776fa398e3c73c453_b.jpg" width="1494"></figure><p>当我们使用较大权重时，不加入BN的网络在一开始就失效；而加入BN的网络能够克服如此bad的权重初始化，并达到接近80%的准确率。</p><h2>2.6 大权重，小学习率，Sigmoid</h2><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-34aad448c6d019164e0f5f544cdf688e_b.jpg" data-caption="" data-size="normal" data-rawwidth="1516" data-rawheight="1144" class="origin_image zh-lightbox-thumb" width="1516" data-original="https://pic3.zhimg.com/v2-34aad448c6d019164e0f5f544cdf688e_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1516' height='1144'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1516" data-rawheight="1144" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic3.zhimg.com/v2-34aad448c6d019164e0f5f544cdf688e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-34aad448c6d019164e0f5f544cdf688e_b.jpg" width="1516"></figure><p>同样使用较大的权重初始化，当我们激活函数为sigmoid时，不加入BN的网络在一开始的准确率有所上升，但随着训练的进行网络逐渐失效，最终准确率仅有30%；而加入BN的网络依旧出色地克服如此bad的权重初始化，并达到接近85%的准确率。</p><h2>2.7 大权重，大学习率，ReLU</h2><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-8126b9bb9b4ccd5648bb46c76d2aa053_b.jpg" data-caption="" data-size="normal" data-rawwidth="1572" data-rawheight="1142" class="origin_image zh-lightbox-thumb" width="1572" data-original="https://pic4.zhimg.com/v2-8126b9bb9b4ccd5648bb46c76d2aa053_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1572' height='1142'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1572" data-rawheight="1142" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-8126b9bb9b4ccd5648bb46c76d2aa053_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-8126b9bb9b4ccd5648bb46c76d2aa053_b.jpg" width="1572"></figure><p>当权重与学习率都很大时，BN网络开始还会训练一段时间，但随后就直接停止训练；而没有BN的神经网络开始就失效。</p><h2>2.8 大权重，大学习率，Sigmoid</h2><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-f509733dbe0b3f0dd7d169dd116e17e8_b.jpg" data-caption="" data-size="normal" data-rawwidth="1524" data-rawheight="1144" class="origin_image zh-lightbox-thumb" width="1524" data-original="https://pic1.zhimg.com/v2-f509733dbe0b3f0dd7d169dd116e17e8_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1524' height='1144'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1524" data-rawheight="1144" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic1.zhimg.com/v2-f509733dbe0b3f0dd7d169dd116e17e8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f509733dbe0b3f0dd7d169dd116e17e8_b.jpg" width="1524"></figure><p>可以看到，加入BN对较大的权重与较大学习率都具有非常好的鲁棒性，最终模型能够达到93%的准确率；而未加入BN的网络则经过一段时间震荡后开始失效。</p><p class="ztext-empty-paragraph"><br></p><p>8个模型的准确率统计如下：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-e2549ae6af18e5a4c3f0656a5ec9a1ab_b.jpg" data-caption="" data-size="normal" data-rawwidth="1592" data-rawheight="592" class="origin_image zh-lightbox-thumb" width="1592" data-original="https://pic4.zhimg.com/v2-e2549ae6af18e5a4c3f0656a5ec9a1ab_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1592' height='592'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1592" data-rawheight="592" class="origin_image zh-lightbox-thumb lazy" data-original="https://pic4.zhimg.com/v2-e2549ae6af18e5a4c3f0656a5ec9a1ab_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e2549ae6af18e5a4c3f0656a5ec9a1ab_b.jpg" width="1592"></figure><p class="ztext-empty-paragraph"><br></p><h2>总结</h2><p>至此，关于Batch Normalization的理论与实战部分就介绍道这里。<b>总
的来说，BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal 
Covariate 
Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂
度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正
则化的效果。</b></p><p class="ztext-empty-paragraph"><br></p><p><b>参考资料：</b></p><p>[1]
 Ioffe S, Szegedy C. Batch normalization: accelerating deep network 
training by reducing internal covariate shift[C]// International 
Conference on International Conference on Machine Learning. JMLR.org, 
2015:448-456.</p><p>[2] 吴恩达Cousera Deep Learning课程</p><p>[3] <a href="https://zhuanlan.zhihu.com/p/33173246" class="internal">详解深度学习中的Normalization，不只是BN</a></p><p>[4] <a href="https://www.zhihu.com/question/38102762" class="internal">深度学习中 Batch Normalization为什么效果好？</a></p><p>[5] Udacity Deep Learning Nanodegree</p><p>[6] <a href="https://link.zhihu.com/?target=https%3A//r2rt.com/implementing-batch-normalization-in-tensorflow.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Implementing Batch Normalization in Tensorflow</a></p><p class="ztext-empty-paragraph"><br></p><blockquote>转载请联系作者获得授权。<br>我的知乎：天雨粟<br>我的专栏：<a href="https://zhuanlan.zhihu.com/zhaoyeyu" class="internal">机器不学习</a><br>我的GitHub：<a href="https://link.zhihu.com/?target=https%3A//github.com/NELSONZHAO" class=" wrap external" target="_blank" rel="nofollow noreferrer">NELSONZHAO</a></blockquote><p></p></div></div><div class="ContentItem-time">编辑于 2018-03-27</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19813032&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content">深度学习（Deep Learning）</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content">机器学习</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20032249&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20032249" target="_blank"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content">TensorFlow 学习</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 324.5px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;34879333&quot;}}}"><span><button aria-label="赞同 1711 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 1711</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>92 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div style="opacity: 1;" class="Post-SideActions"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 1712">赞同 1711</div></div></button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover41-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover41-content"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div><div class="Sticky--holder" style="position: static; inset: auto auto 0px 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://www.zhihu.com/column/zhaoyeyu"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content"><img class="Avatar Avatar--medium Avatar--round" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-ee7c55e5b838e4d20894921289fb7979_xs.jpg" srcset="https://pic4.zhimg.com/v2-ee7c55e5b838e4d20894921289fb7979_l.jpg?source=172ae18b 2x" alt="机器不学习" width="40" height="40"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://www.zhihu.com/column/zhaoyeyu"><div class="Popover"><div id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content">机器不学习</div></div></a></h2><div class="ContentItem-meta">专治机器不会学、瞎学、乱学等疑难杂症</div></div><div class="ContentItem-extra"><a href="https://www.zhihu.com/column/zhaoyeyu" type="button" class="Button">进入专栏</a></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1339px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled="disabled" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/43200897" class="PostItem"><div><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-d9950c280fda77a744d0487e3f8baf25_250x0.jpg" srcset="https://pic3.zhimg.com/v2-d9950c280fda77a744d0487e3f8baf25_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="深度学习中的Normalization模型"><h1 class="PostItem-Title">深度学习中的Normalization模型</h1><div class="PostItem-Footer"><span>张俊林</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/33173246" class="PostItem"><div><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-67e63301b77923897960fb4b50a84ed9_250x0.png" srcset="https://pic1.zhimg.com/v2-67e63301b77923897960fb4b50a84ed9_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="详解深度学习中的Normalization，BN/LN/WN"><h1 class="PostItem-Title">详解深度学习中的Normalization，BN/LN/WN</h1><div class="PostItem-Footer"><span>Juliuszh</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/32714733" class="PostItem"><div><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-5022c810d5368fc75d86d7c2b2dc0860_250x0.jpg" srcset="https://pic2.zhimg.com/v2-5022c810d5368fc75d86d7c2b2dc0860_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="深度学习的前戏--梯度下降、反向传播、激活函数"><h1 class="PostItem-Title">深度学习的前戏--梯度下降、反向传播、激活函数</h1><div class="PostItem-Footer"><span>王乐</span><span class="PostItem-FooterTitle">发表于每天都要机...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/38176412" class="PostItem"><div><img src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-43c3f2423a47f10c5833ba5dff4748bb_250x0.jpg" srcset="https://pic1.zhimg.com/v2-43c3f2423a47f10c5833ba5dff4748bb_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="Batch Normalization导读"><h1 class="PostItem-Title">Batch Normalization导读</h1><div class="PostItem-Footer"><span>张俊林</span><span class="PostItem-FooterTitle"></span></div></div></a><button class="PagingButton PagingButton-Next" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{}"><div class="CommentsV2 CommentsV2--withEditor CommentsV2-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">92 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div><div class="CommentsV2-footer CommentEditorV2--normal"><div class="CommentEditorV2-inputWrap"><div class="InputLike CommentEditorV2-input Editable"><div style="min-height: 198px;" class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-e1vu7" style="white-space: pre-wrap;">评论由作者筛选后显示</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-e1vu7" class="notranslate public-DraftEditor-content" role="textbox" spellcheck="true" style="outline: currentcolor none medium; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;" tabindex="0" contenteditable="true"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="e1vu7" data-offset-key="9kil9-0-0"><div data-offset-key="9kil9-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="9kil9-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" style="display: none;" accept="image/jpg,image/jpeg,image/png,image/gif"><div></div></div><div class="CommentEditorV2-inputUpload"><div class="CommentEditorV2-popoverWrap"><div class="Popover CommentEditorV2-inputUpLoad-Icon"><button aria-label="插入表情" data-tooltip="插入表情" data-tooltip-position="bottom" data-tooltip-will-hide-on-click="true" id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content" type="button" class="Button Editable-control Button--plain"><svg class="Zi Zi--Emotion" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M7.523 13.5h8.954c-.228 2.47-2.145 4-4.477 4-2.332 0-4.25-1.53-4.477-4zM12 21a9 9 0 1 1 0-18 9 9 0 0 1 0 18zm0-1.5a7.5 7.5 0 1 0 0-15 7.5 7.5 0 0 0 0 15zm-3-8a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm6 0a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3z"></path></svg></button></div></div></div></div><button type="button" disabled="disabled" class="Button CommentEditorV2-singleButton Button--primary Button--blue">发布</button></div></div><div><div class="CommentListV2"><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/da-hua-84-84"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/da8e974dc_s_002.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg?source=06d4cd63 2x" alt="vagrant" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/da-hua-84-84">vagrant</a></span><span class="CommentItemV2-time">2018-03-24</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">“BN使得模型对网络中的参数不那么敏感”这段u不是下标吗？怎么还对它求导了？不懂啊！忘大神解答！</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover14-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover14-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-61901a9da09b1ea674e50b98a4279202_s.jpg" srcset="https://pic2.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_xs.jpg?source=06d4cd63 2x" alt="天雨粟" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/da-hua-84-84">vagrant</a></span><span class="CommentItemV2-time">2018-03-25</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">这里u是前一层的输出，也是当前层的输入。这里不是下标哈～</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>2</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiang-yang-50-19"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-64ebc2912710ffe67d98833973671c1c_s.jpg" srcset="https://pic4.zhimg.com/v2-64ebc2912710ffe67d98833973671c1c_xs.jpg?source=06d4cd63 2x" alt="向仰" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xiang-yang-50-19">向仰</a></span><span class="CommentItemV2-time">2018-03-25</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>感谢，看完之后感觉有了初步清晰的了解。</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>4</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover16-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover16-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/wei-zhang-99-92"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/291b397f322f30251cb8cceef6134eef_s.jpg" srcset="https://pic2.zhimg.com/291b397f322f30251cb8cceef6134eef_xs.jpg?source=06d4cd63 2x" alt="韦张" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/wei-zhang-99-92">韦张</a></span><span class="CommentItemV2-time">2018-03-26</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">很清晰</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>1</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover17-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover17-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/dao-cao-83-49"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-4769fe2ce6e5d179f382c9c1257d726b_s.jpg" srcset="https://pic1.zhimg.com/v2-4769fe2ce6e5d179f382c9c1257d726b_xs.jpg?source=06d4cd63 2x" alt="稻草" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/dao-cao-83-49">稻草</a></span><span class="CommentItemV2-time">2018-03-26</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>清晰明了，赞，能否结合L1、L2正则化与BN之间进行对比说明呢。</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>6</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover18-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover18-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhu-xin-geng"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/3512611e039de9818fa08f9bf586f96a_s.jpg" srcset="https://pic1.zhimg.com/3512611e039de9818fa08f9bf586f96a_xs.jpg?source=06d4cd63 2x" alt="快跑啊小女孩" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhu-xin-geng">快跑啊小女孩</a></span><span class="CommentItemV2-time">2018-03-26</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">请问一下 实际模型中有许多是不用bn的 bn的有那么多优点那缺点是什么 什么情况下不适用 谢谢</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover19-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover19-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-61901a9da09b1ea674e50b98a4279202_s_002.jpg" srcset="https://pic1.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_xs.jpg?source=06d4cd63 2x" alt="天雨粟" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhu-xin-geng">快跑啊小女孩</a></span><span class="CommentItemV2-time">2018-03-27</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>Hi，这个问题下答案解释比较详细，我搬运一下<a href="https://www.zhihu.com/question/59728870" class="internal">深度学习加速策略BN、WN和LN的联系与区别，各自的优缺点和适用的场景？</a></p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>3</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover20-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover20-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhu-xin-geng"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/3512611e039de9818fa08f9bf586f96a_s.jpg" srcset="https://pic1.zhimg.com/3512611e039de9818fa08f9bf586f96a_xs.jpg?source=06d4cd63 2x" alt="快跑啊小女孩" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhu-xin-geng">快跑啊小女孩</a></span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-time">2018-03-27</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>谢谢！！</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/da8e974dc_s.jpg" srcset="https://pic1.zhimg.com/da8e974dc_xs.jpg?source=06d4cd63 2x" alt="知乎用户" width="24" height="24"></span><span class="UserLink">知乎用户</span><span class="CommentItemV2-time">2018-03-27</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>"Z^l = W^l + b^l"——是不是漏了一个输入x</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover21-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover21-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-61901a9da09b1ea674e50b98a4279202_s_002.jpg" srcset="https://pic1.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_xs.jpg?source=06d4cd63 2x" alt="天雨粟" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-reply">回复</span><span class="UserLink">知乎用户</span><span class="CommentItemV2-time">2018-03-27</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>感谢提醒！已修正!</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover22-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover22-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zengxianfeng"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/da8e974dc_s_002.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg?source=06d4cd63 2x" alt="曾显傻" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zengxianfeng">曾显傻</a></span><span class="CommentItemV2-time">2018-03-27</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>想问一下每层可以单独训练是什么意思，最后不应该还是要由前馈网络一层一层的训练吗</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover23-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover23-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-61901a9da09b1ea674e50b98a4279202_s_002.jpg" srcset="https://pic1.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_xs.jpg?source=06d4cd63 2x" alt="天雨粟" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zengxianfeng">曾显傻</a></span><span class="CommentItemV2-time">2018-03-27</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>这里说每一层单独训练的意思只是对BN的一个intuition，实际中是按照正常的方式训练。我这里想表达的是说因为BN修正了分布，使得后一层不会过于依赖前一层，对网络层实现了解耦，就有点类似每层单独训练的感觉~</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>3</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover24-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover24-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/juan-juan-meng-7"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-0b3aecd3754a35ac7a4c64c3c944dbdb_s.jpg" srcset="https://pic2.zhimg.com/v2-0b3aecd3754a35ac7a4c64c3c944dbdb_xs.jpg?source=06d4cd63 2x" alt="卷萌" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/juan-juan-meng-7">卷萌</a></span><span class="CommentItemV2-time">2018-04-05</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>为什么感觉sigmoid比relu好呢==</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>1</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover25-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover25-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yicun-shu-guang"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-bd12dfbb070945168066652ef60a77f7_s.jpg" srcset="https://pic4.zhimg.com/v2-bd12dfbb070945168066652ef60a77f7_xs.jpg?source=06d4cd63 2x" alt="诶嘛" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yicun-shu-guang">诶嘛</a></span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/juan-juan-meng-7">卷萌</a></span><span class="CommentItemV2-time">2019-12-19</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>这是因为带了BN啊，而且对于不同的任务，具体用哪个激活函数还是需要进行试错的吧</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover26-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover26-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/begeekmyfriend"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-ba7c7a1220f886a9cf5713133a8a7886_s.jpg" srcset="https://pic1.zhimg.com/v2-ba7c7a1220f886a9cf5713133a8a7886_xs.jpg?source=06d4cd63 2x" alt="我的上铺叫路遥" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/begeekmyfriend">我的上铺叫路遥</a></span><span class="CommentItemV2-time">2018-04-12</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>手动感谢！</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover27-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover27-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/hong-shi-2-10"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-9d9573a174afa3cb0b5f6e14f6a59343_s.jpg" srcset="https://pic1.zhimg.com/v2-9d9573a174afa3cb0b5f6e14f6a59343_xs.jpg?source=06d4cd63 2x" alt="猴猴" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/hong-shi-2-10">猴猴</a></span><span class="CommentItemV2-time">2018-04-16</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>你好，我不明白上层网络如何“适应”底层网络的参数变化，“适应”的具体行为是什么？</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>4</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover28-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover28-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/li-jun-bo-1"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-68799cb23eec1cd64733787c37f95865_s.jpg" srcset="https://pic3.zhimg.com/v2-68799cb23eec1cd64733787c37f95865_xs.jpg?source=06d4cd63 2x" alt="为啥非得取个名" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/li-jun-bo-1">为啥非得取个名</a></span><span class="CommentItemV2-time">2018-04-20</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>很全面，谢谢！</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover29-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover29-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/li-jun-bo-1"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-68799cb23eec1cd64733787c37f95865_s_002.jpg" srcset="https://pic4.zhimg.com/v2-68799cb23eec1cd64733787c37f95865_xs.jpg?source=06d4cd63 2x" alt="为啥非得取个名" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/li-jun-bo-1">为啥非得取个名</a></span><span class="CommentItemV2-time">2018-04-21</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>楼
主，你源码里有一行tf.reset_default_graph(),搞不懂什么意思，我把它删掉后，会出现错误You must feed a 
value for placeholder tensor 'Placeholder' with dtype float ，代码里已经初始化了啊，
 为何删除tf.resetdefault_graph()会产生这样的错误？</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>1</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover30-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover30-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/liang-2-14"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/3cd8d4d269b36add5a4d06f42d01a2fb_s.jpg" srcset="https://pic1.zhimg.com/3cd8d4d269b36add5a4d06f42d01a2fb_xs.jpg?source=06d4cd63 2x" alt="lgZzz" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/liang-2-14">lgZzz</a></span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/li-jun-bo-1">为啥非得取个名</a></span><span class="CommentItemV2-time">2018-12-25</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>没报错</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover31-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover31-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/huhugeng"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/040bc14e37610e17f5243d177c8fff6a_s.jpg" srcset="https://pic4.zhimg.com/040bc14e37610e17f5243d177c8fff6a_xs.jpg?source=06d4cd63 2x" alt="呼呼庚撸量化" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/huhugeng">呼呼庚撸量化</a></span><span class="CommentItemV2-time">2018-05-06</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">很清晰，有做过bn在rnn中的效果么</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover32-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover32-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-61901a9da09b1ea674e50b98a4279202_s_002.jpg" srcset="https://pic1.zhimg.com/v2-61901a9da09b1ea674e50b98a4279202_xs.jpg?source=06d4cd63 2x" alt="天雨粟" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zhao-xie-yu-30">天雨粟</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/huhugeng">呼呼庚撸量化</a></span><span class="CommentItemV2-time">2018-05-06</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">暂时还没尝试过～</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover33-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover33-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/allosvm"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-99cffd9f2a588f1a0ff28395cab065c6_s.jpg" srcset="https://pic4.zhimg.com/v2-99cffd9f2a588f1a0ff28395cab065c6_xs.jpg?source=06d4cd63 2x" alt="AlloAllo" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/allosvm">AlloAllo</a></span><span class="CommentItemV2-time">2018-05-09</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>您好，我有一个问题，原文中有一段是关于μB，σB还有xi的求梯度，然后好像是要对这三个进行更新，但我不明白，对xi求梯度进行更新是什么意思？希望能得到您的解答！谢谢</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover34-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover34-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/rokeabbey"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/da8e974dc_s_003.jpg" srcset="https://pic2.zhimg.com/da8e974dc_xs.jpg?source=06d4cd63 2x" alt="君馳" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/rokeabbey">君馳</a></span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/allosvm">AlloAllo</a></span><span class="CommentItemV2-time">2019-10-13</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>我觉得对xi求梯度不是为了更新xi的东西 而是为了更前面的层求梯度，毕竟求梯度是复合求导嘛</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>1</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover35-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover35-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/benying"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-7c2d24e61d51ebc55d27eecbd868a8f0_s.jpg" srcset="https://pic1.zhimg.com/v2-7c2d24e61d51ebc55d27eecbd868a8f0_xs.jpg?source=06d4cd63 2x" alt="benying" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/benying">benying</a></span><span class="CommentItemV2-time">2018-05-29</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>关于BN写的最好最全的文章，适合小白，谢谢</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover36-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover36-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xhpdmc"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/da8e974dc_s_002.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg?source=06d4cd63 2x" alt="江大桥" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xhpdmc">江大桥</a></span><span class="CommentItemV2-time">2018-07-26</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>说清楚了为什么需要BN，这才是最重要的</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover37-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover37-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/hei-bai-jiao-yin"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-301c96caa357649fd13b563fa6e2b474_s.jpg" srcset="https://pic3.zhimg.com/v2-301c96caa357649fd13b563fa6e2b474_xs.jpg?source=06d4cd63 2x" alt="暖玉" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/hei-bai-jiao-yin">暖玉</a></span><span class="CommentItemV2-time">2018-07-31</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>准确率也太低了，，，</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover38-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover38-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yang-xu-dong-6"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/0e3878ca4_s.jpg" srcset="https://pic4.zhimg.com/0e3878ca4_xs.jpg?source=06d4cd63 2x" alt="杨旭东" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yang-xu-dong-6">杨旭东</a></span><span class="CommentItemV2-time">2018-09-04</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>总结得非常好，实验数据也很有意义！赞！</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover39-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover39-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/bu-zhi-dao-jiao-shi-yao-50-22"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-e290c005a6f8e93ca5d315e3445a9a36_s.jpg" srcset="https://pic1.zhimg.com/v2-e290c005a6f8e93ca5d315e3445a9a36_xs.jpg?source=06d4cd63 2x" alt="不知道叫什么" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/bu-zhi-dao-jiao-shi-yao-50-22">不知道叫什么</a></span><span class="CommentItemV2-time">2018-09-20</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">目前我看过解释bn最好的文章了，赞</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover40-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover40-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/csw-dlut"><img class="Avatar UserLink-avatar" src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/v2-06d1a2eb9a4a28072bd9cc503f8457cf_s.jpg" srcset="https://pic2.zhimg.com/v2-06d1a2eb9a4a28072bd9cc503f8457cf_xs.jpg?source=06d4cd63 2x" alt="不理不理" width="24" height="24"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/csw-dlut">不理不理</a></span><span class="CommentItemV2-time">2018-10-23</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>看了两天几乎知乎上所有BN的文章，这篇最好了</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" style="transform: rotate(180deg); margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" style="margin-right: 5px;" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul></div><div class="Pagination CommentsV2-pagination"><button type="button" disabled="disabled" class="Button PaginationButton PaginationButton--current Button--plain">1</button><button type="button" class="Button PaginationButton Button--plain">2</button><button type="button" class="Button PaginationButton Button--plain">3</button><button type="button" class="Button PaginationButton Button--plain">4</button><button type="button" class="Button PaginationButton PaginationButton-next Button--plain">下一页</button></div></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" aria-label="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{},"admins":{"data":[]},"members":{"data":[]},"explore":{"candidateSyncClubs":{}},"profile":{},"checkin":{},"comments":{"paging":{},"loading":{},"meta":{},"ids":{}},"postList":{"paging":{},"loading":{},"ids":{}},"recommend":{"data":[]},"silences":{"data":[]},"application":{"profile":null}},"entities":{"users":{"zhao-xie-yu-30":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202.jpg?source=172ae18b","uid":"558900893454479360","userType":"people","isFollowing":false,"urlToken":"zhao-xie-yu-30","id":"97d929615d936543e9a40895b35fb43c","description":"","name":"天雨粟","isAdvertiser":false,"headline":"计算广告\u002FCTR\u002F算法\u002F工程","gender":1,"url":"\u002Fpeople\u002F97d929615d936543e9a40895b35fb43c","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b","isOrg":false,"type":"people","levelInfo":{"exp":80470,"level":7,"nicknameColor":{"color":"","nightModeColor":""},"levelIcon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-925f3f81dab3105fd42b785305a1cc28_l.png","iconInfo":{"url":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-925f3f81dab3105fd42b785305a1cc28_l.png","nightModeUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4e20c1d51a61ffb91eebdc40e420e382_l.png","width":93,"height":51}},"badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_l.png?source=172ae18b","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"34879333":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"id":34879333,"title":"Batch Normalization原理与实战","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F34879333","imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3126b47b329a0141944b11ede097a8f_720w.jpg?source=172ae18b","titleImage":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3126b47b329a0141944b11ede097a8f_720w.jpg?source=172ae18b","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"1052\" data-watermark=\"\" data-original-src=\"\" data-watermark-src=\"\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_r.jpg\"\u002F\u003E好久没有更新专栏了，从去年6月开始一直在忙实习，年初实习结束了又在写毕业论文，终于搞的差不多了，可以抽空来慢慢更新专栏内容了！ 前言本期专栏主要来从理论与实战视角对深度学习中的Batch Normalization的思路进行讲解、归纳和总结，并辅以代码让小伙…","created":1521876074,"updated":1522165083,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202.jpg?source=172ae18b","uid":"558900893454479360","userType":"people","isFollowing":false,"urlToken":"zhao-xie-yu-30","id":"97d929615d936543e9a40895b35fb43c","description":"","name":"天雨粟","isAdvertiser":false,"headline":"计算广告\u002FCTR\u002F算法\u002F工程","gender":1,"url":"\u002Fpeople\u002F97d929615d936543e9a40895b35fb43c","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b","isOrg":false,"type":"people","levelInfo":{"exp":80470,"level":7,"nicknameColor":{"color":"","nightModeColor":""},"levelIcon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-925f3f81dab3105fd42b785305a1cc28_l.png","iconInfo":{"url":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-925f3f81dab3105fd42b785305a1cc28_l.png","nightModeUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4e20c1d51a61ffb91eebdc40e420e382_l.png","width":93,"height":51}},"badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_l.png?source=172ae18b","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"censor","copyrightPermission":"need_review","state":"published","imageWidth":1860,"imageHeight":464,"content":"\u003Cp\u003E好久没有更新专栏了，从去年6月开始一直在忙实习，年初实习结束了又在写毕业论文，终于搞的差不多了，可以抽空来慢慢更新专栏内容了！\u003C\u002Fp\u003E\u003Ch2\u003E前言\u003C\u002Fh2\u003E\u003Cp\u003E本期专栏主要来从理论与实战视角对深度学习中的Batch Normalization的思路进行讲解、归纳和总结，并辅以代码让小伙伴儿们对Batch Normalization的作用有更加直观的了解。\u003C\u002Fp\u003E\u003Cp\u003E本文主要分为两大部分。\u003Cb\u003E第一部分是理论板块\u003C\u002Fb\u003E，主要从背景、算法、效果等角度对Batch Normalization进行详解；\u003Cb\u003E第二部分是实战板块\u003C\u002Fb\u003E，主要以MNIST数据集作为整个代码测试的数据，通过比较加入Batch Normalization前后网络的性能来让大家对Batch Normalization的作用与效果有更加直观的感知。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E（一）理论板块\u003C\u002Fh2\u003E\u003Cp\u003E理论板块将从以下四个方面对Batch Normalization进行详解：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E提出背景\u003C\u002Fli\u003E\u003Cli\u003EBN算法思想\u003C\u002Fli\u003E\u003Cli\u003E测试阶段如何使用BN\u003C\u002Fli\u003E\u003Cli\u003EBN的优势\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E理论部分主要参考2015年Google的Sergey Ioffe与Christian Szegedy的论文内容，并辅以吴恩达Coursera课程与其它博主的资料。所有参考内容链接均见于文章最后参考链接部分。\u003C\u002Fp\u003E\u003Ch2\u003E1 提出背景\u003C\u002Fh2\u003E\u003Ch2\u003E1.1 炼丹的困扰\u003C\u002Fh2\u003E\u003Cp\u003E在深度学习中，由于问题的复杂性，我们往往会使用较深层数的网络进行训练，相信很多炼丹的朋友都对调参的困难有所体会，尤其是对深层神经网络的训练调参更是困难且复杂。在这个过程中，我们需要去尝试不同的学习率、初始化参数方法（例如Xavier初始化）等方式来帮助我们的模型加速收敛。深度神经网络之所以如此难训练，其中一个重要原因就是网络中层与层之间存在高度的关联性与耦合性。下图是一个多层的神经网络，层与层之间采用全连接的方式进行连接。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"1052\" class=\"origin_image zh-lightbox-thumb\" width=\"1694\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1694&#39; height=&#39;1052&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"1052\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1694\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E我们规定左侧为神经网络的底层，右侧为神经网络的上层。那么网络中层与层之间的关联性会导致如下的状况：随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。\u003C\u002Fp\u003E\u003Ch2\u003E1.2 什么是Internal Covariate Shift\u003C\u002Fh2\u003E\u003Cp\u003EBatch Normalization的原论文作者给了Internal Covariate Shift一个较规范的定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。\u003C\u002Fp\u003E\u003Cp\u003E这句话该怎么理解呢？我们同样以1.1中的图为例，我们定义每一层的线性变换为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7D%5Ctimes+input%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}=W^{[l]}\\times input+b^{[l]}\" eeimg=\"1\"\u002F\u003E，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l+\" alt=\"l \" eeimg=\"1\"\u002F\u003E 代表层数；非线性变换为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"A^{[l]}=g^{[l]}(Z^{[l]})\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g%5E%7B%5Bl%5D%7D%28%5Ccdot%29\" alt=\"g^{[l]}(\\cdot)\" eeimg=\"1\"\u002F\u003E 为第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的激活函数。\u003C\u002Fp\u003E\u003Cp\u003E随着梯度下降的进行，每一层的参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]}\" eeimg=\"1\"\u002F\u003E 与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"\u002F\u003E 都会被更新，那么 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"\u002F\u003E 的分布也就发生了改变，进而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"\u002F\u003E 也同样出现分布的改变。而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"\u002F\u003E 作为第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l%2B1\" alt=\"l+1\" eeimg=\"1\"\u002F\u003E 层的输入，意味着 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l%2B1\" alt=\"l+1\" eeimg=\"1\"\u002F\u003E 层就需要去不停适应这种数据分布的变化，这一过程就被叫做Internal Covariate Shift。\u003C\u002Fp\u003E\u003Ch2\u003E1.3 Internal Covariate Shift会带来什么问题？\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003E（1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E我们在上面提到了梯度下降的过程会让每一层的参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"\u002F\u003E 发生变化，进而使得每一层的线性与非线性计算结果分布产生变化。后层网络就要不停地去适应这种分布变化，这个时候就会使得整个网络的学习速率过慢。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E当我们在神经网络中采用饱和激活函数（saturated activation function）时，例如sigmoid，tanh激活函数，很容易使得模型训练陷入梯度饱和区（saturated regime）。随着模型训练的进行，我们的参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]}\" eeimg=\"1\"\u002F\u003E 会逐渐更新并变大，此时 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\" eeimg=\"1\"\u002F\u003E 就会随之变大，并且 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"\u002F\u003E 还受到更底层网络参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%5E%7B%5B1%5D%7D%2CW%5E%7B%5B2%5D%7D%2C%5Ccdots%2CW%5E%7B%5Bl-1%5D%7D\" alt=\"W^{[1]},W^{[2]},\\cdots,W^{[l-1]}\" eeimg=\"1\"\u002F\u003E 的影响，随着网络层数的加深， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"\u002F\u003E 很容易陷入梯度饱和区，此时梯度会变得很小甚至接近于0，参数的更新速度就会减慢，进而就会放慢网络的收敛速度。\u003C\u002Fp\u003E\u003Cp\u003E对于激活函数梯度饱和问题，有两种解决思路。第一种就是更为非饱和性激活函数，例如线性整流函数ReLU可以在一定程度上解决训练进入梯度饱和区的问题。另一种思路是，我们可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。\u003C\u002Fp\u003E\u003Ch2\u003E1.4 我们如何减缓Internal Covariate Shift？\u003C\u002Fh2\u003E\u003Cp\u003E要缓解ICS的问题，就要明白它产生的原因。ICS产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可以通过固定每一层网络输入值的分布来对减缓ICS问题。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（1）白化（Whitening）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E白化（Whitening）是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cb\u003E使得输入特征分布具有相同的均值与方差。\u003C\u002Fb\u003E其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E去除特征之间的相关性。\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛（LeCun et al.,1998b；Wiesler&amp;Ney,2011）。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（2）Batch Normalization提出\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E既然白化可以解决这个问题，为什么我们还要提出别的解决办法？当然是现有的方法具有一定的缺陷，白化主要有以下两个问题：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cb\u003E白化过程计算成本太高，\u003C\u002Fb\u003E并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作；\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E白化过程由于改变了网络每一层的分布\u003C\u002Fb\u003E，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E既然有了上面两个问题，那我们的解决思路就很简单，一方面，我们提出的normalization方法要能够简化计算过程；另一方面又需要经过规范化处理后让数据尽可能保留原始的表达能力。于是就有了简化+改进版的白化——Batch Normalization。\u003C\u002Fp\u003E\u003Ch2\u003E2 Batch Normalization\u003C\u002Fh2\u003E\u003Ch2\u003E2.1 思路\u003C\u002Fh2\u003E\u003Cp\u003E既然白化计算过程比较复杂，那我们就简化一点，比如我们可以尝试单独对每个特征进行normalizaiton就可以了，让每个特征都有均值为0，方差为1的分布就OK。\u003C\u002Fp\u003E\u003Cp\u003E另一个问题，既然白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。\u003C\u002Fp\u003E\u003Cp\u003E因此，基于上面两个解决问题的思路，作者提出了Batch Normalization，下一部分来具体讲解这个算法步骤。\u003C\u002Fp\u003E\u003Ch2\u003E2.2 算法\u003C\u002Fh2\u003E\u003Cp\u003E在深度学习中，由于采用full batch的训练方式对内存要求较大，且每一轮训练时间过长；我们一般都会采用对数据做划分，用mini-batch对网络进行训练。因此，Batch Normalization也就在mini-batch的基础上进行计算。\u003C\u002Fp\u003E\u003Ch2\u003E2.2.1 参数定义\u003C\u002Fh2\u003E\u003Cp\u003E我们依旧以下图这个神经网络为例。我们定义网络总共有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L\" alt=\"L\" eeimg=\"1\"\u002F\u003E 层（不包含输入层）并定义如下符号：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"1052\" class=\"origin_image zh-lightbox-thumb\" width=\"1694\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1694&#39; height=&#39;1052&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"1052\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1694\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c3934c77a543b6c588af07a365f9a70b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E参数相关：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E ：网络中的层标号\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L\" alt=\"L\" eeimg=\"1\"\u002F\u003E ：网络中的最后一层或总层数\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d_l\" alt=\"d_l\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的维度，即神经元结点数\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]}\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的权重矩阵， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%5E%7B%5Bl%5D%7D%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_l%5Ctimes+d_%7Bl-1%7D%7D\" alt=\"W^{[l]}\\in \\mathbb{R}^{d_l\\times d_{l-1}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的偏置向量， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b%5E%7B%5Bl%5D%7D%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_l%5Ctimes+1%7D\" alt=\"b^{[l]}\\in \\mathbb{R}^{d_l\\times 1}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的线性计算结果， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7D%5Ctimes+input%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}=W^{[l]}\\times input+b^{[l]}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g%5E%7B%5Bl%5D%7D%28%5Ccdot%29\" alt=\"g^{[l]}(\\cdot)\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的激活函数\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的非线性激活结果， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"A^{[l]}=g^{[l]}(Z^{[l]})\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cb\u003E样本相关：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=M\" alt=\"M\" eeimg=\"1\"\u002F\u003E ：训练样本的数量\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=N\" alt=\"N\" eeimg=\"1\"\u002F\u003E ：训练样本的特征数\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X\" alt=\"X\" eeimg=\"1\"\u002F\u003E ：训练样本集， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%3D%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C%5Ccdots%2Cx%5E%7B%28M%29%7D%5C%7D%EF%BC%8CX%5Cin+%5Cmathbb%7BR%7D%5E%7BN%5Ctimes+M%7D\" alt=\"X=\\{x^{(1)},x^{(2)},\\cdots,x^{(M)}\\}，X\\in \\mathbb{R}^{N\\times M}\" eeimg=\"1\"\u002F\u003E （注意这里 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X\" alt=\"X\" eeimg=\"1\"\u002F\u003E 的一列是一个样本）\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m\" alt=\"m\" eeimg=\"1\"\u002F\u003E ：batch size，即每个batch中样本的数量\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cchi%5E%7B%28i%29%7D\" alt=\"\\chi^{(i)}\" eeimg=\"1\"\u002F\u003E ：第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i\" alt=\"i\" eeimg=\"1\"\u002F\u003E 个mini-batch的训练数据， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%3D+%5C%7B%5Cchi%5E%7B%281%29%7D%2C%5Cchi%5E%7B%282%29%7D%2C%5Ccdots%2C%5Cchi%5E%7B%28k%29%7D%5C%7D\" alt=\"X= \\{\\chi^{(1)},\\chi^{(2)},\\cdots,\\chi^{(k)}\\}\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cchi%5E%7B%28i%29%7D%5Cin+%5Cmathbb%7BR%7D%5E%7BN%5Ctimes+m%7D\" alt=\"\\chi^{(i)}\\in \\mathbb{R}^{N\\times m}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E2.2.2 算法步骤\u003C\u002Fh2\u003E\u003Cp\u003E介绍算法思路沿袭前面BN提出的思路来讲。第一点，对每个特征进行独立的normalization。我们考虑一个batch的训练，传入m个训练样本，并关注网络中的某一层，忽略上标 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_l%5Ctimes+m%7D\" alt=\"Z\\in \\mathbb{R}^{d_l\\times m}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E我们关注当前层的第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E 个维度，也就是第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E 个神经元结点，则有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z_j%5Cin+%5Cmathbb%7BR%7D%5E%7B1%5Ctimes+m%7D\" alt=\"Z_j\\in \\mathbb{R}^{1\\times m}\" eeimg=\"1\"\u002F\u003E 。我们当前维度进行规范化：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_j%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em+Z_j%5E%7B%28i%29%7D\" alt=\"\\mu_j=\\frac{1}{m}\\sum_{i=1}^m Z_j^{(i)}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_j%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28Z_j%5E%7B%28i%29%7D-%5Cmu_j%29%5E2\" alt=\"\\sigma^2_j=\\frac{1}{m}\\sum_{i=1}^m(Z_j^{(i)}-\\mu_j)^2\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BZ%7D_j%3D%5Cfrac%7BZ_j-%5Cmu_j%7D%7B%5Csqrt%7B%5Csigma_j%5E2%2B%5Cepsilon%7D%7D\" alt=\"\\hat{Z}_j=\\frac{Z_j-\\mu_j}{\\sqrt{\\sigma_j^2+\\epsilon}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cblockquote\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 是为了防止方差为0产生无效计算。\u003C\u002Fblockquote\u003E\u003Cp\u003E下面我们再来结合个具体的例子来进行计算。下图我们只关注第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的计算结果，左边的矩阵是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\" eeimg=\"1\"\u002F\u003E 线性计算结果，还未进行激活函数的非线性变换。此时每一列是一个样本，图中可以看到共有8列，代表当前训练样本的batch中共有8个样本，每一行代表当前 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层神经元的一个节点，可以看到当前 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层共有4个神经元结点，即第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层维度为4。我们可以看到，每行的数据分布都不同。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-084e9875d10896369e09af5a60e56250_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2176\" data-rawheight=\"1140\" class=\"origin_image zh-lightbox-thumb\" width=\"2176\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-084e9875d10896369e09af5a60e56250_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2176&#39; height=&#39;1140&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2176\" data-rawheight=\"1140\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2176\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-084e9875d10896369e09af5a60e56250_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-084e9875d10896369e09af5a60e56250_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E对于第一个神经元，我们求得 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_1%3D1.65\" alt=\"\\mu_1=1.65\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_1%3D0.44\" alt=\"\\sigma^2_1=0.44\" eeimg=\"1\"\u002F\u003E （其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon%3D10%5E%7B-8%7D\" alt=\"\\epsilon=10^{-8}\" eeimg=\"1\"\u002F\u003E ），此时我们利用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_1%2C%5Csigma%5E2_1\" alt=\"\\mu_1,\\sigma^2_1\" eeimg=\"1\"\u002F\u003E 对第一行数据（第一个维度）进行normalization得到新的值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5B-0.98%2C-0.23%2C-0.68%2C-1.13%2C0.08%2C0.68%2C2.19%2C0.08%5D\" alt=\"[-0.98,-0.23,-0.68,-1.13,0.08,0.68,2.19,0.08]\" eeimg=\"1\"\u002F\u003E 。同理我们可以计算出其他输入维度归一化后的值。如下图：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c37bda8f138402cc7c3dd62c509d36f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2266\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb\" width=\"2266\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c37bda8f138402cc7c3dd62c509d36f6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2266&#39; height=&#39;1142&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2266\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2266\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c37bda8f138402cc7c3dd62c509d36f6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c37bda8f138402cc7c3dd62c509d36f6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E通过上面的变换，\u003Cb\u003E我们解决了第一个问题，即用更加简化的方式来对数据进行规范化，使得第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层的输入每个特征的分布均值为0，方差为1。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E如同上面提到的，Normalization操作我们虽然缓解了ICS问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E因此，BN又引入了两个可学习（learnable）的参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"\u002F\u003E 。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BZ_j%7D%3D%5Cgamma_j+%5Chat%7BZ%7D_j%2B%5Cbeta_j\" alt=\"\\tilde{Z_j}=\\gamma_j \\hat{Z}_j+\\beta_j\" eeimg=\"1\"\u002F\u003E 。特别地，当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma%5E2%3D%5Csigma%5E2%2C%5Cbeta%3D%5Cmu\" alt=\"\\gamma^2=\\sigma^2,\\beta=\\mu\" eeimg=\"1\"\u002F\u003E 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E通过上面的步骤，我们就在一定程度上保证了输入数据的表达能力。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E以上就是整个Batch Normalization在模型训练中的算法和思路。\u003C\u002Fp\u003E\u003Cblockquote\u003E补充： 在进行normalization的过程中，由于我们的规范化操作会对减去均值，因此，偏置项 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b\" alt=\"b\" eeimg=\"1\"\u002F\u003E 可以被忽略掉或可以被置为0，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=BN%28Wu%2Bb%29%3DBN%28Wu%29\" alt=\"BN(Wu+b)=BN(Wu)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fblockquote\u003E\u003Ch2\u003E2.2.3 公式\u003C\u002Fh2\u003E\u003Cp\u003E对于神经网络中的第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l\" alt=\"l\" eeimg=\"1\"\u002F\u003E 层，我们有：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5EmZ%5E%7B%5Bl%5D%28i%29%7D\" alt=\"\\mu=\\frac{1}{m}\\sum_{i=1}^mZ^{[l](i)}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28Z%5E%7B%5Bl%5D%28i%29%7D-%5Cmu%29%5E2\" alt=\"\\sigma^2=\\frac{1}{m}\\sum_{i=1}^m(Z^{[l](i)}-\\mu)^2\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BZ%5E%7B%5Bl%5D%7D-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta\" alt=\"\\tilde{Z}^{[l]}=\\gamma\\cdot\\frac{Z^{[l]}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%5E%7B%5Bl%5D%7D%3Dg%5E%7B%5Bl%5D%7D%28%5Ctilde%7BZ%7D%5E%7B%5Bl%5D%7D%29\" alt=\"A^{[l]}=g^{[l]}(\\tilde{Z}^{[l]})\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E3 测试阶段如何使用Batch Normalization？\u003C\u002Fh2\u003E\u003Cp\u003E我们知道BN在每一层计算的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2\" alt=\"\\sigma^2\" eeimg=\"1\"\u002F\u003E 都是基于当前batch中的训练数据，但是这就带来了一个问题：我们在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，此时 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2\" alt=\"\\sigma^2\" eeimg=\"1\"\u002F\u003E 的计算一定是有偏估计，这个时候我们该如何进行计算呢？\u003C\u002Fp\u003E\u003Cp\u003E利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_%7Bbatch%7D\" alt=\"\\mu_{batch}\" eeimg=\"1\"\u002F\u003E 与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_%7Bbatch%7D\" alt=\"\\sigma^2_{batch}\" eeimg=\"1\"\u002F\u003E 。此时我们使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_%7Btest%7D%3D%5Cmathbb%7BE%7D+%28%5Cmu_%7Bbatch%7D%29\" alt=\"\\mu_{test}=\\mathbb{E} (\\mu_{batch})\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_%7Btest%7D%3D%5Cfrac%7Bm%7D%7Bm-1%7D%5Cmathbb%7BE%7D%28%5Csigma%5E2_%7Bbatch%7D%29\" alt=\"\\sigma^2_{test}=\\frac{m}{m-1}\\mathbb{E}(\\sigma^2_{batch})\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E得到每个特征的均值与方差的无偏估计后，我们对test数据采用同样的normalization方法：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=BN%28X_%7Btest%7D%29%3D%5Cgamma%5Ccdot+%5Cfrac%7BX_%7Btest%7D-%5Cmu_%7Btest%7D%7D%7B%5Csqrt%7B%5Csigma%5E2_%7Btest%7D%2B%5Cepsilon%7D%7D%2B%5Cbeta\" alt=\"BN(X_{test})=\\gamma\\cdot \\frac{X_{test}-\\mu_{test}}{\\sqrt{\\sigma^2_{test}+\\epsilon}}+\\beta\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E另外，除了采用整体样本的无偏估计外。吴恩达在Coursera上的Deep Learning课程指出可以对train阶段每个batch计算的mean\u002Fvariance采用指数加权平均来得到test阶段mean\u002Fvariance的估计。\u003C\u002Fp\u003E\u003Ch2\u003E4 Batch Normalization的优势\u003C\u002Fh2\u003E\u003Cp\u003EBatch Normalization在实际工程中被证明了能够缓解神经网络难以训练的问题，BN具有的有事可以总结为以下三点：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EBN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。\u003C\u002Fp\u003E\u003Cp\u003E当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。例如，我们对参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W\" alt=\"W\" eeimg=\"1\"\u002F\u003E 进行缩放得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=aW\" alt=\"aW\" eeimg=\"1\"\u002F\u003E 。对于缩放前的值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Wu\" alt=\"Wu\" eeimg=\"1\"\u002F\u003E ，我们设其均值为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_1\" alt=\"\\mu_1\" eeimg=\"1\"\u002F\u003E ，方差为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_1\" alt=\"\\sigma^2_1\" eeimg=\"1\"\u002F\u003E ；对于缩放值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=aWu\" alt=\"aWu\" eeimg=\"1\"\u002F\u003E ，设其均值为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_2\" alt=\"\\mu_2\" eeimg=\"1\"\u002F\u003E ，方差为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_2\" alt=\"\\sigma^2_2\" eeimg=\"1\"\u002F\u003E ，则我们有：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_2%3Da%5Cmu_1\" alt=\"\\mu_2=a\\mu_1\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma%5E2_2%3Da%5E2%5Csigma%5E2_1\" alt=\"\\sigma^2_2=a^2\\sigma^2_1\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E我们忽略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E ，则有：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=BN%28aWu%29%3D%5Cgamma%5Ccdot%5Cfrac%7BaWu-%5Cmu_2%7D%7B%5Csqrt%7B%5Csigma%5E2_2%7D%7D%2B%5Cbeta%3D%5Cgamma%5Ccdot%5Cfrac%7BaWu-a%5Cmu_1%7D%7B%5Csqrt%7Ba%5E2%5Csigma%5E2_1%7D%7D%2B%5Cbeta%3D%5Cgamma%5Ccdot%5Cfrac%7BWu-%5Cmu_1%7D%7B%5Csqrt%7B%5Csigma%5E2_1%7D%7D%2B%5Cbeta%3DBN%28Wu%29\" alt=\"BN(aWu)=\\gamma\\cdot\\frac{aWu-\\mu_2}{\\sqrt{\\sigma^2_2}}+\\beta=\\gamma\\cdot\\frac{aWu-a\\mu_1}{\\sqrt{a^2\\sigma^2_1}}+\\beta=\\gamma\\cdot\\frac{Wu-\\mu_1}{\\sqrt{\\sigma^2_1}}+\\beta=BN(Wu)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial%7BBN%28%28aW%29u%29%7D%7D%7B%5Cpartial%7Bu%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BaW%7D%7B%5Csqrt%7B%5Csigma%5E2_2%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7BaW%7D%7B%5Csqrt%7Ba%5E2%5Csigma%5E2_1%7D%7D%3D%5Cfrac%7B%5Cpartial%7BBN%28Wu%29%7D%7D%7B%5Cpartial%7Bu%7D%7D\" alt=\"\\frac{\\partial{BN((aW)u)}}{\\partial{u}}=\\gamma\\cdot\\frac{aW}{\\sqrt{\\sigma^2_2}}=\\gamma\\cdot\\frac{aW}{\\sqrt{a^2\\sigma^2_1}}=\\frac{\\partial{BN(Wu)}}{\\partial{u}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B%5Cpartial%7BBN%28%28aW%29u%29%7D%7D%7B%5Cpartial%7B%28aW%29%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7Bu%7D%7B%5Csqrt%7B%5Csigma%5E2_2%7D%7D%3D%5Cgamma%5Ccdot%5Cfrac%7Bu%7D%7Ba%5Csqrt%7B%5Csigma%5E2_1%7D%7D%3D%5Cfrac%7B1%7D%7Ba%7D%5Ccdot%5Cfrac%7B%5Cpartial%7BBN%28Wu%29%7D%7D%7B%5Cpartial%7BW%7D%7D\" alt=\"\\frac{\\partial{BN((aW)u)}}{\\partial{(aW)}}=\\gamma\\cdot\\frac{u}{\\sqrt{\\sigma^2_2}}=\\gamma\\cdot\\frac{u}{a\\sqrt{\\sigma^2_1}}=\\frac{1}{a}\\cdot\\frac{\\partial{BN(Wu)}}{\\partial{W}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cblockquote\u003E注：公式中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=u\" alt=\"u\" eeimg=\"1\"\u002F\u003E 是当前层的输入，也是前一层的输出；不是下标啊旁友们！\u003C\u002Fblockquote\u003E\u003Cp\u003E我们可以看到，经过BN操作以后，权重的缩放值会被“抹去”，因此保证了输入数据分布稳定在一定范围内。另外，权重的缩放并不会影响到对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=u\" alt=\"u\" eeimg=\"1\"\u002F\u003E 的梯度计算；并且当权重越大时，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 越大， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B1%7D%7Ba%7D\" alt=\"\\frac{1}{a}\" eeimg=\"1\"\u002F\u003E 越小，意味着权重 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W\" alt=\"W\" eeimg=\"1\"\u002F\u003E 的梯度反而越小，这样BN就保证了梯度不会依赖于参数的scale，使得参数的更新处在更加稳定的状态。\u003C\u002Fp\u003E\u003Cp\u003E因此，在使用Batch Normalization之后，抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强，此时我们可以设置较大的学习率而不用过于担心模型divergence的风险。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"\u002F\u003E 又让数据保留更多的原始信息。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E（4）BN具有一定的正则化效果\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。\u003C\u002Fp\u003E\u003Cp\u003E另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E（二）实战板块\u003C\u002Fh2\u003E\u003Cp\u003E经过了上面了理论学习，我们对BN有了理论上的认知。“Talk is cheap, show me the code”。接下来我们就通过实际的代码来对比加入BN前后的模型效果。实战部分使用MNIST数据集作为数据基础，并使用TensorFlow中的Batch Normalization结构来进行BN的实现。\u003C\u002Fp\u003E\u003Cp\u003E数据准备：MNIST手写数据集\u003C\u002Fp\u003E\u003Cp\u003E代码地址：我的\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002FNELSONZHAO\u002Fzhihu\u002Ftree\u002Fmaster\u002Fbatch_normalization_discussion\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGitHub\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E注：TensorFlow版本为1.6.0\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E实战板块主要分为两部分：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E网络构建与辅助函数\u003C\u002Fli\u003E\u003Cli\u003EBN测试\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E1 网络构建与辅助函数\u003C\u002Fh2\u003E\u003Cp\u003E首先我们先定义一下神经网络的类，这个类里面主要包括了以下方法：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003Ebuild_network：前向计算\u003C\u002Fli\u003E\u003Cli\u003Efully_connected：全连接计算\u003C\u002Fli\u003E\u003Cli\u003Etrain：训练模型\u003C\u002Fli\u003E\u003Cli\u003Etest：测试模型\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E1.1 build_network\u003C\u002Fh2\u003E\u003Cp\u003E我们首先通过构造函数，把权重、激活函数以及是否使用BN这些变量传入，并生成一个training_accuracies来记录训练过程中的模型准确率变化。这里的initial_weights是一个list，list中每一个元素是一个矩阵（二维tuple），存储了每一层的权重矩阵。build_network实现了网络的构建，并调用了fully_connected函数（下面会提）进行计算。要注意的是，由于MNIST是多分类，在这里我们不需要对最后一层进行激活，保留计算的logits就好。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-11f333fecc38621169bfa37d52e35bc2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1760\" data-rawheight=\"1226\" class=\"origin_image zh-lightbox-thumb\" width=\"1760\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-11f333fecc38621169bfa37d52e35bc2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1760&#39; height=&#39;1226&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1760\" data-rawheight=\"1226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1760\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-11f333fecc38621169bfa37d52e35bc2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-11f333fecc38621169bfa37d52e35bc2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E1.2 fully_connected\u003C\u002Fh2\u003E\u003Cp\u003E这里的fully_connected主要用来每一层的线性与非线性计算。通过self.use_batch_norm来控制是否使用BN。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-cbea7e029dc0820d0ff69bbbef9fc995_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1992\" data-rawheight=\"748\" class=\"origin_image zh-lightbox-thumb\" width=\"1992\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-cbea7e029dc0820d0ff69bbbef9fc995_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1992&#39; height=&#39;748&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1992\" data-rawheight=\"748\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1992\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-cbea7e029dc0820d0ff69bbbef9fc995_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-cbea7e029dc0820d0ff69bbbef9fc995_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E另外，值得注意的是，tf.layers.batch_normalization接口中training参数非常重要，官方文档中描述为：\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cb\u003E\u003Ccode\u003Etraining\u003C\u002Fcode\u003E\u003C\u002Fb\u003E: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). \u003Cb\u003ENOTE\u003C\u002Fb\u003E: make sure to set this parameter correctly, or else your training\u002Finference will not work properly.\u003C\u002Fblockquote\u003E\u003Cp\u003E当我们训练时，要设置为True，保证在训练过程中使用的是mini-batch的统计量进行normalization；在Inference阶段，使用False，也就是使用总体样本的无偏估计。\u003C\u002Fp\u003E\u003Ch2\u003E1.3 train\u003C\u002Fh2\u003E\u003Cp\u003Etrain函数主要用来进行模型的训练。除了要定义label，loss以及optimizer以外，我们还需要注意，官方文档指出在使用BN时的事项：\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cb\u003ENote:\u003C\u002Fb\u003E when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in \u003Ccode\u003Etf.GraphKeys.UPDATE_OPS\u003C\u002Fcode\u003E, so they need to be added as a dependency to the \u003Ccode\u003Etrain_op\u003C\u002Fcode\u003E. \u003C\u002Fblockquote\u003E\u003Cp\u003E因此当self.use_batch_norm为True时，要使用tf.control_dependencies保证模型正常训练。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dd2cfbaadd03f1f35b9f77a5ac570d75_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1988\" data-rawheight=\"1636\" class=\"origin_image zh-lightbox-thumb\" width=\"1988\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dd2cfbaadd03f1f35b9f77a5ac570d75_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1988&#39; height=&#39;1636&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1988\" data-rawheight=\"1636\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1988\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dd2cfbaadd03f1f35b9f77a5ac570d75_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dd2cfbaadd03f1f35b9f77a5ac570d75_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cblockquote\u003E注意：在训练过程中batch_size选了60（mnist.train.next_batch(60)），这里是因为BN的原paper中用的60。( We trained the network for 50000 steps, with 60 examples per mini-batch.)\u003C\u002Fblockquote\u003E\u003Ch2\u003E1.4 test\u003C\u002Fh2\u003E\u003Cp\u003Etest阶段与train类似，只是要设置self.is_training=False，保证Inference阶段BN的正确。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8f42ba5dc8755655bc1d5ad36ae6d0e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1668\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb\" width=\"1668\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8f42ba5dc8755655bc1d5ad36ae6d0e1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1668&#39; height=&#39;672&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1668\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1668\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8f42ba5dc8755655bc1d5ad36ae6d0e1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8f42ba5dc8755655bc1d5ad36ae6d0e1_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E经过上面的步骤，我们的框架基本就搭好了，接下来我们再写一个辅助函数train_and_test以及plot绘图函数就可以开始对BN进行测试啦。train_and_test以及plot函数见GitHub代码中，这里不再赘述。\u003C\u002Fp\u003E\u003Ch2\u003E2 BN测试\u003C\u002Fh2\u003E\u003Cp\u003E在这里，我们构造一个4层神经网络，输入层结点数784，三个隐层均为128维，输出层10个结点，如下图所示：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a9555e40a7395054b24e0c8a2d89a578_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"1344\" class=\"origin_image zh-lightbox-thumb\" width=\"1362\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a9555e40a7395054b24e0c8a2d89a578_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1362&#39; height=&#39;1344&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"1344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1362\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a9555e40a7395054b24e0c8a2d89a578_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a9555e40a7395054b24e0c8a2d89a578_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E实验中，我们主要控制一下三个变量：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E权重矩阵（较小初始化权重，标准差为0.05；较大初始化权重，标准差为10）\u003C\u002Fli\u003E\u003Cli\u003E学习率（较小学习率：0.01；较大学习率：2）\u003C\u002Fli\u003E\u003Cli\u003E隐层激活函数（relu，sigmoid）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E2.1 小权重，小学习率，ReLU\u003C\u002Fh2\u003E\u003Cp\u003E测试结果如下图：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1db5b7e223ef0e4688e233f3e3f30c51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1546\" data-rawheight=\"1156\" class=\"origin_image zh-lightbox-thumb\" width=\"1546\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1db5b7e223ef0e4688e233f3e3f30c51_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1546&#39; height=&#39;1156&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1546\" data-rawheight=\"1156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1546\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1db5b7e223ef0e4688e233f3e3f30c51_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1db5b7e223ef0e4688e233f3e3f30c51_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E我们可以得到以下结论：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E在训练与预测阶段，加入BN的模型准确率都稍高一点；\u003C\u002Fli\u003E\u003Cli\u003E加入BN的网络收敛更快（黄线）\u003C\u002Fli\u003E\u003Cli\u003E没有加入BN的网络训练速度更快（483.61it\u002Fs&gt;329.23it\u002Fs），这是因为BN增加了神经网络中的计算量\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E为了更清楚地看到BN收敛速度更快，我们把减少Training batches，设置为3000，得到如下结果：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-04ed801797f69120fc601933b48aee28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1526\" data-rawheight=\"1164\" class=\"origin_image zh-lightbox-thumb\" width=\"1526\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-04ed801797f69120fc601933b48aee28_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1526&#39; height=&#39;1164&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1526\" data-rawheight=\"1164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1526\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-04ed801797f69120fc601933b48aee28_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-04ed801797f69120fc601933b48aee28_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从上图中我们就可以清晰看到，加入BN的网络在第500个batch的时候已经能够在validation数据集上达到90%的准确率；而没有BN的网络的准确率还在不停波动，并且到第3000个batch的时候才达到90%的准确率。\u003C\u002Fp\u003E\u003Ch2\u003E2.2 小权重，小学习率，Sigmoid\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-dd09009ee3937fa0a4268b7b0eb22397_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"1156\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-dd09009ee3937fa0a4268b7b0eb22397_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1602&#39; height=&#39;1156&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"1156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-dd09009ee3937fa0a4268b7b0eb22397_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-dd09009ee3937fa0a4268b7b0eb22397_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E学习率与权重均没变，我们把隐层激活函数换为sigmoid。可以发现，BN收敛速度非常之快，而没有BN的网络前期在不断波动，直到第20000个train batch以后才开始进入平稳的训练状态。\u003C\u002Fp\u003E\u003Ch2\u003E2.3 小权重，大学习率，ReLU\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-899b1d8fb2c372a7a6829adc10aab0f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-899b1d8fb2c372a7a6829adc10aab0f0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1558&#39; height=&#39;1142&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-899b1d8fb2c372a7a6829adc10aab0f0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-899b1d8fb2c372a7a6829adc10aab0f0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在本次实验中，我们使用了较大的学习率，较大的学习率意味着权重的更新跨度很大，而根据我们前面理论部分的介绍，BN不会受到权重scale的影响，因此其能够使模型保持在一个稳定的训练状态；而没有加入BN的网络则在一开始就由于学习率过大导致训练失败。\u003C\u002Fp\u003E\u003Ch2\u003E2.4 小权重，大学习率，Sigmoid\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-50bd080b121e64376c2e818f9651bdec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1512\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb\" width=\"1512\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-50bd080b121e64376c2e818f9651bdec_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1512&#39; height=&#39;1144&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1512\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1512\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-50bd080b121e64376c2e818f9651bdec_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-50bd080b121e64376c2e818f9651bdec_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在保持较大学习率（learning rate=2）的情况下，当我们将激活函数换为sigmoid以后，两个模型都能够达到一个很好的效果，并且在test数据及上的准确率非常接近；但加入BN的网络要收敛地更快，同样的，我们来观察3000次batch的训练准确率。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40dec0842bb3f78024514ae0fae2c3cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"1156\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40dec0842bb3f78024514ae0fae2c3cf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1556&#39; height=&#39;1156&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"1156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1556\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40dec0842bb3f78024514ae0fae2c3cf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40dec0842bb3f78024514ae0fae2c3cf_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当我们把training batch限制到3000以后，可以发现加入BN后，尽管我们使用较大的学习率，其仍然能够在大约500个batch以后在validation上达到90%的准确率；但不加入BN的准确率前期在一直大幅度波动，到大约1000个batch以后才达到90%的准确率。\u003C\u002Fp\u003E\u003Ch2\u003E2.5 大权重，小学习率，ReLU\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c17a9620dc1f6ee776fa398e3c73c453_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1494\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb\" width=\"1494\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c17a9620dc1f6ee776fa398e3c73c453_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1494&#39; height=&#39;1144&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1494\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1494\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c17a9620dc1f6ee776fa398e3c73c453_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c17a9620dc1f6ee776fa398e3c73c453_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当我们使用较大权重时，不加入BN的网络在一开始就失效；而加入BN的网络能够克服如此bad的权重初始化，并达到接近80%的准确率。\u003C\u002Fp\u003E\u003Ch2\u003E2.6 大权重，小学习率，Sigmoid\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-34aad448c6d019164e0f5f544cdf688e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1516\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb\" width=\"1516\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-34aad448c6d019164e0f5f544cdf688e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1516&#39; height=&#39;1144&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1516\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1516\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-34aad448c6d019164e0f5f544cdf688e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-34aad448c6d019164e0f5f544cdf688e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E同样使用较大的权重初始化，当我们激活函数为sigmoid时，不加入BN的网络在一开始的准确率有所上升，但随着训练的进行网络逐渐失效，最终准确率仅有30%；而加入BN的网络依旧出色地克服如此bad的权重初始化，并达到接近85%的准确率。\u003C\u002Fp\u003E\u003Ch2\u003E2.7 大权重，大学习率，ReLU\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8126b9bb9b4ccd5648bb46c76d2aa053_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb\" width=\"1572\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8126b9bb9b4ccd5648bb46c76d2aa053_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1572&#39; height=&#39;1142&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1572\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8126b9bb9b4ccd5648bb46c76d2aa053_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8126b9bb9b4ccd5648bb46c76d2aa053_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当权重与学习率都很大时，BN网络开始还会训练一段时间，但随后就直接停止训练；而没有BN的神经网络开始就失效。\u003C\u002Fp\u003E\u003Ch2\u003E2.8 大权重，大学习率，Sigmoid\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f509733dbe0b3f0dd7d169dd116e17e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1524\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb\" width=\"1524\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f509733dbe0b3f0dd7d169dd116e17e8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1524&#39; height=&#39;1144&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1524\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1524\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f509733dbe0b3f0dd7d169dd116e17e8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f509733dbe0b3f0dd7d169dd116e17e8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以看到，加入BN对较大的权重与较大学习率都具有非常好的鲁棒性，最终模型能够达到93%的准确率；而未加入BN的网络则经过一段时间震荡后开始失效。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E8个模型的准确率统计如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2549ae6af18e5a4c3f0656a5ec9a1ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"1592\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2549ae6af18e5a4c3f0656a5ec9a1ab_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1592&#39; height=&#39;592&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1592\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2549ae6af18e5a4c3f0656a5ec9a1ab_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2549ae6af18e5a4c3f0656a5ec9a1ab_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E总结\u003C\u002Fh2\u003E\u003Cp\u003E至此，关于Batch Normalization的理论与实战部分就介绍道这里。\u003Cb\u003E总的来说，BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean\u002Fvariance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E参考资料：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E[1] Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]\u002F\u002F International Conference on International Conference on Machine Learning. JMLR.org, 2015:448-456.\u003C\u002Fp\u003E\u003Cp\u003E[2] 吴恩达Cousera Deep Learning课程\u003C\u002Fp\u003E\u003Cp\u003E[3] \u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F33173246\" class=\"internal\"\u003E详解深度学习中的Normalization，不只是BN\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E[4] \u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F38102762\" class=\"internal\"\u003E深度学习中 Batch Normalization为什么效果好？\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E[5] Udacity Deep Learning Nanodegree\u003C\u002Fp\u003E\u003Cp\u003E[6] \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fr2rt.com\u002Fimplementing-batch-normalization-in-tensorflow.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EImplementing Batch Normalization in Tensorflow\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cblockquote\u003E转载请联系作者获得授权。\u003Cbr\u002F\u003E我的知乎：天雨粟\u003Cbr\u002F\u003E我的专栏：\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fzhaoyeyu\" class=\"internal\"\u003E机器不学习\u003C\u002Fa\u003E\u003Cbr\u002F\u003E我的GitHub：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002FNELSONZHAO\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ENELSONZHAO\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","type":"topic","id":"19813032","name":"深度学习（Deep Learning）"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20032249","type":"topic","id":"20032249","name":"TensorFlow 学习"}],"voteupCount":1711,"voting":0,"column":{"description":"不定期分享机器学习与深度学习实战代码。","canManage":false,"intro":"专治机器不会学、瞎学、乱学等疑难杂症","isFollowing":false,"urlToken":"zhaoyeyu","id":"zhaoyeyu","articlesCount":16,"acceptSubmission":true,"title":"机器不学习","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fzhaoyeyu","commentPermission":"all","created":1495721799,"updated":1591377492,"imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ee7c55e5b838e4d20894921289fb7979_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202.jpg?source=172ae18b","uid":"558900893454479360","userType":"people","isFollowing":false,"urlToken":"zhao-xie-yu-30","id":"97d929615d936543e9a40895b35fb43c","description":"","name":"天雨粟","isAdvertiser":false,"headline":"计算广告\u002FCTR\u002F算法\u002F工程","gender":1,"url":"\u002Fpeople\u002F97d929615d936543e9a40895b35fb43c","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":6483,"type":"column"},"commentCount":92,"contributions":[{"id":1186530,"state":"accepted","type":"first_publish","column":{"description":"不定期分享机器学习与深度学习实战代码。","canManage":false,"intro":"专治机器不会学、瞎学、乱学等疑难杂症","isFollowing":false,"urlToken":"zhaoyeyu","id":"zhaoyeyu","articlesCount":16,"acceptSubmission":true,"title":"机器不学习","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fzhaoyeyu","commentPermission":"all","created":1495721799,"updated":1591377492,"imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ee7c55e5b838e4d20894921289fb7979_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202.jpg?source=172ae18b","uid":"558900893454479360","userType":"people","isFollowing":false,"urlToken":"zhao-xie-yu-30","id":"97d929615d936543e9a40895b35fb43c","description":"","name":"天雨粟","isAdvertiser":false,"headline":"计算广告\u002FCTR\u002F算法\u002F工程","gender":1,"url":"\u002Fpeople\u002F97d929615d936543e9a40895b35fb43c","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":6483,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":4,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"Batch Normalization原理与实战 - 来自知乎专栏「机器不学习」，作者: 天雨粟 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F34879333 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":"请输入评论，您的评论将会由作者筛选后显示"},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":49,"visibleOnlyToAuthor":false,"hasColumn":true,"republishers":[]}},"columns":{"zhaoyeyu":{"description":"不定期分享机器学习与深度学习实战代码。","canManage":false,"intro":"专治机器不会学、瞎学、乱学等疑难杂症","isFollowing":false,"urlToken":"zhaoyeyu","id":"zhaoyeyu","articlesCount":16,"acceptSubmission":true,"title":"机器不学习","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fzhaoyeyu","commentPermission":"all","created":1495721799,"updated":1591377492,"imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ee7c55e5b838e4d20894921289fb7979_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202.jpg?source=172ae18b","uid":"558900893454479360","userType":"people","isFollowing":false,"urlToken":"zhao-xie-yu-30","id":"97d929615d936543e9a40895b35fb43c","description":"","name":"天雨粟","isAdvertiser":false,"headline":"计算广告\u002FCTR\u002F算法\u002F工程","gender":1,"url":"\u002Fpeople\u002F97d929615d936543e9a40895b35fb43c","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-61901a9da09b1ea674e50b98a4279202_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":6483,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-qa_cl_guest-2","expPrefix":"qa_cl_guest","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_profile_video-11","expPrefix":"vd_profile_video","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_timeguide-2","expPrefix":"vd_timeguide","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_video_replay-3","expPrefix":"vd_video_replay","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"mov_recom-1_v1","expPrefix":"mov_recom","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"Test_Punk-1_v2","expPrefix":"Test_Punk","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"general_1-2_v1","expPrefix":"general_1","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"web_scl_rec","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"gue_v_serial","type":"String","value":"1","layerId":"guevd_layer_695"},{"id":"gue_bulletmb","type":"String","value":"0","layerId":"guevd_layer_812"},{"id":"gue_playh_an","type":"String","value":"0","layerId":"guevd_layer_999"},{"id":"test_ret","type":"Int","value":"1","chainId":"_gene_","layerId":"test_ret"},{"id":"gue_repost","type":"String","value":"0","layerId":"gueqa_layer_671"},{"id":"ge_recall","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1029","key":3110},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_","layerId":"lili_layer_11"},{"id":"ge_spe_rt","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":2970},{"id":"ge_corr","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_976","key":3041},{"id":"tp_clubhyb","type":"String","value":"1","chainId":"_all_","layerId":"tptp_layer_619"},{"id":"ge_prf_rec","type":"String","value":"0","chainId":"_gene_","layerId":"getop_layer_991","key":3040},{"id":"gue_video_replay","type":"String","value":"2","layerId":"guevd_layer_3"},{"id":"ge_cupboard","type":"String","value":"1","chainId":"_gene_","layerId":"geli_layer_948","key":3001},{"id":"gue_self_censoring","type":"String","value":"1","layerId":"gueqa_layer_1"},{"id":"gue_card_test","type":"String","value":"1","layerId":"gueqa_layer_2"},{"id":"ge_video","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":2831},{"id":"ge_item","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_945","key":2971},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_","layerId":"iosus_layer_1"},{"id":"gue_video_guide","type":"String","value":"1","layerId":"guevd_layer_625"},{"id":"web_mem_tab","type":"String","value":"0","layerId":"webtop_layer_1006"},{"id":"web_heifetz_grow_ad","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_2"},{"id":"web_sem_ab","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"web_login","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"web_answerlist_ad","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"ge_recency","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3072},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"mov_recom","type":"Int","value":"100","layerId":"mov_recom"},{"id":"gue_fo_recom","type":"String","value":"0","layerId":"gueqa_layer_780"},{"id":"ge_vector","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_989","key":3134},{"id":"gue_zvideo_title","type":"String","value":"0","layerId":"guevd_layer_559"},{"id":"gue_iosplay","type":"String","value":"0","layerId":"guevd_layer_997"},{"id":"gue_andplayd","type":"String","value":"0","layerId":"guevd_layer_998"},{"id":"web_img_up","type":"String","value":"0","layerId":"webqa_layer_862"},{"id":"ge_club_ai","type":"String","value":"1","chainId":"_gene_","layerId":"getp_layer_827","key":2950},{"id":"ge_flow_join","type":"String","value":"0","chainId":"_gene_","layerId":"getp_layer_872","key":2988},{"id":"gue_bullet_second","type":"String","value":"1","layerId":"guevd_layer_1"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_","layerId":"zrrec_layer_2"},{"id":"ge_rm_d2q_v2","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1034","key":3105},{"id":"gue_profile_video","type":"String","value":"1","layerId":"guevd_layer_5"},{"id":"gue_cdzixun","type":"String","value":"0","layerId":"gueqa_layer_3"},{"id":"gue_art_sani","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"web_column_auto_invite","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"web_audit_01","type":"String","value":"case1","layerId":"webre_layer_1"},{"id":"ge_newcard","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_839","key":2997},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_718"},{"id":"web_ad_banner","type":"String","value":"0","layerId":"webgw_layer_3"},{"id":"ge_guess","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_938","key":2912},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_9"},{"id":"ge_rec_2th","type":"String","value":"11","chainId":"_gene_","layerId":"geli_layer_965","key":3023},{"id":"li_sp_mqbk","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_748"},{"id":"tp_zrec","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_619"},{"id":"gue_q_intercept","type":"String","value":"0","layerId":"gueqa_layer_2"},{"id":"ge_v_rank_v3","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":2966},{"id":"ge_com_sup","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_859","key":2891},{"id":"ge_kocbox","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_945","key":3087},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_","layerId":"tsptop_layer_1"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_11"},{"id":"ge_usercard1","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":2740},{"id":"ge_relation2","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_815","key":2796},{"id":"pf_profile2_tab","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_601"},{"id":"gue_zvideo_link","type":"String","value":"1","layerId":"guevd_layer_2"},{"id":"ge_entity","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_946","key":3036},{"id":"ge_newyanzhi","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_1019","key":2788},{"id":"gue_article_sicon","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"gue_share_icon","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"web_answer_list_ad","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"web_unfriendly_comm","type":"String","value":"0","layerId":"webre_layer_1"},{"id":"ge_mixer","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1009","key":3106},{"id":"ge_hard_s_ma","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":3031},{"id":"gue_push2follow","type":"String","value":"1","layerId":"gueqa_layer_3"},{"id":"gue_art_ui","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"ge_profile3","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_1019","key":3104},{"id":"ge_dipin_pre","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3124},{"id":"ge_meta_ss","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_834","key":3079},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…","layerId":"guevd_layer_0"},{"id":"Test_Punk","type":"Int","value":"0","layerId":"Test_Punk"},{"id":"general_1","type":"Int","value":"2","chainId":"_gene_","layerId":"general_1"},{"id":"li_panswer_topic","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_602"},{"id":"web_collection_guest","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"ge_sxzx","type":"String","value":"0","chainId":"_gene_","layerId":"gere_layer_990","key":3060},{"id":"gue_vid_tab","type":"String","value":"0","layerId":"guevd_layer_900"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_3"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_7"},{"id":"gue_q_share","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"tp_contents","type":"String","value":"2","chainId":"_all_","layerId":"tptp_layer_627"},{"id":"ge_cp_zvideo","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":3111},{"id":"ge_upload","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_839","key":2892},{"id":"li_edu_page","type":"String","value":"old","chainId":"_all_","layerId":"lili_layer_580"},{"id":"gue_recmess","type":"String","value":"0","layerId":"gueqa_layer_795"},{"id":"ge_search_ui","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_838","key":2898},{"id":"tp_dingyue_video","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"gue_goods_card","type":"String","value":"0","layerId":"gueqa_layer_1"},{"id":"gue_visit_n_artcard","type":"String","value":"1","layerId":"gueqa_layer_579"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"gue_sharp","type":"String","value":"1","layerId":"guevd_layer_686"},{"id":"ls_video_commercial","type":"String","value":"0","chainId":"_all_","layerId":"lsvd_layer_7"},{"id":"gue_art2qa","type":"String","value":"0","layerId":"gueqa_layer_579"},{"id":"gue_messrec","type":"String","value":"0","layerId":"gueqa_layer_769"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_5"},{"id":"ge_mixer2","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1034","key":3133},{"id":"web_creator_route","type":"String","value":"1","layerId":"webtop_layer_1"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"ge_v_rank_v2","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_904","key":2904},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_","layerId":"sese_layer_4"},{"id":"ge_ocr","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1009","key":3059},{"id":"ge_yuzhi_v1","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1029","key":3127},{"id":"ge_infinity6","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_815","key":2817}],"chains":[{"chainId":"_all_"}],"encodedParams":"CkgmDJoL4QvgC7kLDwubCwAMPgyGC6wLIQy1C2ALzwuWC0sLDwy0CuwK3AvkCiIM1wsgDDQMBwz0CycMTAtSCz0MWAvzCzcMAQsSJAAAAQABAAAAAAEAAQAACwAAAAABAAAAAAAAAAAAAAEAAAAAAA=="},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko\u002F20100101 Firefox\u002F81.0"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F34879333","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F34879333","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":false,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"baiduSearch":false,"googleSearch":true,"miniProgram":false,"xiaomi":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fwww.google.com\u002F","xUDID":"AICa4uoE8hGPTjlTKkE7yXOZkfM7FViVnEY=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"深圳","countryName":"中国","regionName":"广东","countryCode":"CN"},"logged":false,"vars":{"xAppVersion":"","xAppZa":"","xMsId":"","xZst81":"","xZst82":""}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{},"videoSupport":{}},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"room":{"meta":{},"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"zhaoyeyu"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0},"visible":false}},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{}},"zvideos":{"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"club":{}},"republish":{}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/vendor.js"></script><script src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/column_006.js"></script><script src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/hm.js" async=""></script><script src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/zap.js"></script><script src="Batch%20Normalization%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98%20-%20%E7%9F%A5%E4%B9%8E_files/push.js"></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><div><div><div class="css-8pdeid"></div></div></div><div><div><div style="left: -1179px; top: -999px;" class="Editable-languageSuggestions"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete12-0" id="Popover11-toggle" aria-haspopup="true" aria-owns="Popover11-content" class="Input" placeholder="选择语言"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></label></div></div></div></div></div></body></html>