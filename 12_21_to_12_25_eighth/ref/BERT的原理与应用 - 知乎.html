<!DOCTYPE html>
<!-- saved from url=(0038)https://zhuanlan.zhihu.com/p/101570806 -->
<html lang="zh" data-hairline="true" data-theme="light" data-react-helmet="data-theme"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>BERT的原理与应用 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="keywords" content="BERT,自然语言处理,Transformer"><meta data-react-helmet="true" name="description" content="NLP目录阿力阿哩哩：NLP目录我们在第七章介绍了迁移学习与计算机视觉的故事，不过好故事并没有这么快结束。迁移学习一路前行，走进了自然语言处理的片场。迁移学习在自然语言处理（NLP）领域同样也是一种强大的技…"><meta data-react-helmet="true" property="og:title" content="BERT的原理与应用"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/101570806"><meta data-react-helmet="true" property="og:description" content="NLP目录阿力阿哩哩：NLP目录我们在第七章介绍了迁移学习与计算机视觉的故事，不过好故事并没有这么快结束。迁移学习一路前行，走进了自然语言处理的片场。迁移学习在自然语言处理（NLP）领域同样也是一种强大的技…"><meta data-react-helmet="true" property="og:image" content="https://pic1.zhimg.com/v2-88b281667a417305b70bcc826a831f9c_720w.jpg?source=172ae18b"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./BERT的原理与应用 - 知乎_files/column.app.216a26f4.7c0478e2a4904f313332.css" rel="stylesheet"><script defer="" crossorigin="anonymous" src="./BERT的原理与应用 - 知乎_files/init.js.下載" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;1370-72d8f261&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><script charset="utf-8" src="./BERT的原理与应用 - 知乎_files/column.zswsdid.fd69a457857f64c34f22.js.下載"></script><link rel="stylesheet" type="text/css" href="./BERT的原理与应用 - 知乎_files/column.user-hover-card.216a26f4.5e4f61c99a9dfa0f2aae.css"><script charset="utf-8" src="./BERT的原理与应用 - 知乎_files/column.user-hover-card.92c183fc9cf18dd64605.js.下載"></script><link rel="stylesheet" type="text/css" href="./BERT的原理与应用 - 知乎_files/column.Labels.216a26f4.27aa30197fb7b6d3264b.css"><script charset="utf-8" src="./BERT的原理与应用 - 知乎_files/column.Labels.06c15d5fbd26d531a06b.js.下載"></script><link rel="stylesheet" type="text/css" href="./BERT的原理与应用 - 知乎_files/column.modals.216a26f4.4e7b86cdf157acceca01.css"><script charset="utf-8" src="./BERT的原理与应用 - 知乎_files/column.modals.4ec6ebee1b7d62adb839.js.下載"></script><link rel="stylesheet" type="text/css" href="./BERT的原理与应用 - 知乎_files/column.comments-modals.216a26f4.f9aa32b59944f3f131f3.css"><script charset="utf-8" src="./BERT的原理与应用 - 知乎_files/column.comments-modals.0fe5eb8c51608c04bdaf.js.下載"></script><style data-emotion="css"></style><link rel="stylesheet" type="text/css" href="./BERT的原理与应用 - 知乎_files/column.richinput.216a26f4.7df79f851d9e713fb9c1.css"><script charset="utf-8" src="./BERT的原理与应用 - 知乎_files/column.richinput.e3d5bfce93810c5cf2ae.js.下載"></script></head><body class="WhiteBg-body" data-react-helmet="class" style=""><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;阿力阿哩哩&quot;,&quot;itemId&quot;:101570806,&quot;title&quot;:&quot;BERT的原理与应用&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;101570806&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1360px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a class="ZhihuLogoLink" href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://www.zhihu.com/column/c_1173558548797038592">通俗易懂NLP</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div><div class="Sticky--holder" style="position: relative; inset: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><img class="TitleImage" src="./BERT的原理与应用 - 知乎_files/v2-88b281667a417305b70bcc826a831f9c_1440w.jpg" alt="BERT的原理与应用"><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">BERT的原理与应用</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="阿力阿哩哩"><meta itemprop="image" content="https://pic2.zhimg.com/v2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b"><meta itemprop="url" content="https://www.zhihu.com/people/bie-ying-xiang-zhi-li"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/bie-ying-xiang-zhi-li"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./BERT的原理与应用 - 知乎_files/v2-bf1176d84800efad9719a57d03ca92d3_xs.jpg" srcset="https://pic2.zhimg.com/v2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b 2x" alt="阿力阿哩哩"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/bie-ying-xiang-zhi-li">阿力阿哩哩</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">NLPer，微信公众号同名【阿力阿哩哩】</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">33 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2>NLP目录</h2><a target="_blank" href="https://zhuanlan.zhihu.com/p/107833571" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-c4b3ca216c48b9d11d204061f97aa12d_180x120.jpg" data-image-width="1937" data-image-height="791" class="LinkCard LinkCard--hasImage" data-za-detail-view-id="172"><span class="LinkCard-backdrop" style="background-image:url(https://pic2.zhimg.com/v2-c4b3ca216c48b9d11d204061f97aa12d_180x120.jpg)"></span><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">阿力阿哩哩：NLP目录</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"></path></svg></span>zhuanlan.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="./BERT的原理与应用 - 知乎_files/v2-c4b3ca216c48b9d11d204061f97aa12d_180x120.jpg"></span></span></a><p>我们在第七章介绍了迁移学习与计算机视觉的故事，不过好故事并没有这么快结束。迁移学习一路前行，走进了自然语言处理的片场。迁移学习在自然语言处理（NLP）领域同样也是一种强大的技术。由这种技术训练出来的模型，我们称之为预训练模型。<br></p><p>预训练模型首先要针对数据丰富的任务进行预训练，然后再针对下游任务进行微调，以达到下游任务的最佳效果。迁移学习的有效性引起了理论和实践的多样性，人们通过迁移学习与自然语言处理两者相结合，高效地完成了各种NLP的实际任务。</p><h2>8.1 自然语言处理预训练模型</h2><p>使语言建模和其他学习问题变得困难的一个基本问题是维数的诅咒。在人们想要对许多离散的随机变量（例如句子中的单词或数据挖掘任务中的离散属性）之间的联合分布建模时，这一点尤其明显。</p><p>举个例子，假如我们有10000个单词的词汇表，我们要对它们进行离散表示，这样用one-hot 编码整个词汇表就需要10000*10000的矩阵，而one-hot编码矩阵存在很多“0”值，显然浪费了绝大部分的内存空间。为了解决维度诅咒带来的问题，人们开始使用低维度的向量空间来表示单词，从而减少运算资源的损耗，这也是预训练模型思想的开端。</p><h2>8.1.1 Word2Vec</h2><p>在4.7.2节的实验中{<a href="https://zhuanlan.zhihu.com/p/106961648" class="internal" data-za-detail-view-id="1043">Word2Vec｜Skip-gram</a>}，我们提及了Skip-gram模型，它就是Yoshua Bengio等人[1]提出的经典Word2Vec模型之一。Word2Vec模型对NLP任务的效果有显著的提升，并且能够利用更长的上下文。对于Word2Vec具体的原理与应用，笔者在4.7.2节已经进行了详细的讲解，而且本章的主角并不是它，所以笔者就不进行赘述了。</p><a target="_blank" href="https://zhuanlan.zhihu.com/p/106961648" data-draft-node="block" data-draft-type="link-card" data-image="https://pic4.zhimg.com/v2-a2e792d91abbc2dc2b8b68690b287d0f_180x120.jpg" data-image-width="720" data-image-height="388" class="LinkCard LinkCard--hasImage" data-za-detail-view-id="172"><span class="LinkCard-backdrop" style="background-image:url(https://pic4.zhimg.com/v2-a2e792d91abbc2dc2b8b68690b287d0f_180x120.jpg)"></span><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">阿力阿哩哩：Word2Vec｜Skip-gram</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"></path></svg></span>zhuanlan.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="./BERT的原理与应用 - 知乎_files/v2-a2e792d91abbc2dc2b8b68690b287d0f_180x120.jpg"></span></span></a><h2>8.1.2 BERT</h2><p>在2018年，什么震惊了NLP学术界？毫无疑问是Jacob Devlin等人[2]提出的预训练模型BERT（Bidirectional Encoder Representations from Transformers）。</p><p>BERT被设计为通过在所有层的双向上下文上共同进行条件化来预训练未标记文本的深层双向表示。我们可以在仅一个附加输出层的情况下对经过预训练的BERT模型进行微调，以创建适用于各种任务（例如问题解答和语言推断）的最新模型，进而减少了对NLP任务精心设计特定体系结构的需求。BERT是第一个基于微调的表示模型，可在一系列句子级和字符级任务上实现最先进的性能，优于许多特定于任务的体系结构。</p><p>通俗易懂来讲就是我们只需要把BERT当成一个深层次的Word2Vec预训练模型，对于一些特定的任务，我们只需要在BERT之后下接一些网络结构就可以出色地完成这些任务。</p><p>另外，2018年底提出的BERT推动了11项NLP任务的发展。BERT的结构是来自Transformers模型的Encoder，Transformers如图 8.1所示。我们从图X中可以看到Transformer的内部结构都是由Ashish Vaswani 等人[3]提出的Self-Attention Layer和Layer Normalization的堆叠而产生。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-6f01f69b5693df76cebd8aec73c8ec4c_b.jpg" data-size="normal" data-rawwidth="537" data-rawheight="785" class="origin_image zh-lightbox-thumb" width="537" data-original="https://pic1.zhimg.com/v2-6f01f69b5693df76cebd8aec73c8ec4c_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-6f01f69b5693df76cebd8aec73c8ec4c_720w.jpg" data-size="normal" data-rawwidth="537" data-rawheight="785" class="origin_image zh-lightbox-thumb lazy" width="537" data-original="https://pic1.zhimg.com/v2-6f01f69b5693df76cebd8aec73c8ec4c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-6f01f69b5693df76cebd8aec73c8ec4c_b.jpg" data-lazy-status="ok"><figcaption>图 8.1 Transformers（左边为Encoder，右边为Decoder）</figcaption></figure><p><b>1. Self-Attention Layer原理</b></p><p>1) Self-Attention Layer的出现原因</p><p>为了解决RNN、LSTM等常用于处理序列化数据的网络结构无法在GPU中并行加速计算的问题。</p><p>2) Self-Attention Layer的输入</p><p>如图8.2所示：将输入的Input转化成token embedding + segment embedding +position embedding。因为有时候训练样本是由两句话组成，因此“[CLS]”是用来分类输入的两句话是否有上下文关系，而“[SEP]”则是用以分开两句话的标志符。其中，因为这里的Input是英文单词，所以在灌入模型之前，需要用BERT源码的Tokenization工具对每一个单词进行分词，分词后的形式如图 8.2中Input的“Playing”转换成“Play”+“# #ing”，因为英文词汇表是通过词根与词缀的组合来新增单词语义的，所以我们选择用分词方法可以减少整体的词汇表长度。如果是中文字符的话，输入就不需要分词，整段话的每一个字用“空格”隔开即可。</p><p>值得注意的是，模型是无法处理文本字符的，所以不管是英文还是中文，我们都需要通过预训练模型BERT自带的字典vocab.txt将每一个字或者单词转换成字典索引（即id）输入。</p><p>(1) segment embedding的目的：有些任务是两句话一起放入输入X，而segment便是用来区分这两句话的。在Input那里就是用“[SEP]”作为标志符号。而“[CLS]”用来分类输入的两句话是否有上下文关系。</p><p>(2) position embedding的目的：因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding。构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding：使用正弦函数，位置维度对应曲线，而且方便序列之间的选对位置，使用正弦会比余弦好的原因是可以在训练过程中，将原本序列外拓成比原来序列还要长的序列，如公式（8.1）~（8.2）所示。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_b.jpg" data-caption="" data-size="normal" data-rawwidth="706" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="706" data-original="https://pic1.zhimg.com/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_720w.jpg" data-caption="" data-size="normal" data-rawwidth="706" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="706" data-original="https://pic1.zhimg.com/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_b.jpg" data-lazy-status="ok"></figure><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-b097414710b1f094ecc3fe421afd275f_b.jpg" data-size="normal" data-rawwidth="984" data-rawheight="321" class="origin_image zh-lightbox-thumb" width="984" data-original="https://pic4.zhimg.com/v2-b097414710b1f094ecc3fe421afd275f_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-b097414710b1f094ecc3fe421afd275f_720w.jpg" data-size="normal" data-rawwidth="984" data-rawheight="321" class="origin_image zh-lightbox-thumb lazy" width="984" data-original="https://pic4.zhimg.com/v2-b097414710b1f094ecc3fe421afd275f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b097414710b1f094ecc3fe421afd275f_b.jpg" data-lazy-status="ok"><figcaption>图 8.2 Self-Attention的输入</figcaption></figure><p>首先，将 Q与K 矩阵乘积并scale（为了防止结果过大，除以他们维度的均方根），其次，将其灌入Softmax函数得到概率分布，最后再与V 矩阵相乘，得到self-attention的输出，如公式（8.3）所示。其中，(Q,K,V) 均来自同一输入X ，他们是 X分别乘上 WQ,WK,WV初始化权值矩阵所得，而后这三个权值矩阵会在训练的过程中确定下来，如图 8.3所示。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-f03dbc9f1ef3ad994cf12cbabd0a80ab_b.jpg" data-caption="" data-size="normal" data-rawwidth="367" data-rawheight="89" class="content_image" width="367"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-f03dbc9f1ef3ad994cf12cbabd0a80ab_720w.jpg" data-caption="" data-size="normal" data-rawwidth="367" data-rawheight="89" class="content_image lazy" width="367" data-actualsrc="https://pic4.zhimg.com/v2-f03dbc9f1ef3ad994cf12cbabd0a80ab_b.jpg" data-lazy-status="ok"></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-95aad6469883b2244f7888f9447ed47c_b.jpg" data-size="normal" data-rawwidth="1200" data-rawheight="500" class="origin_image zh-lightbox-thumb" width="1200" data-original="https://pic1.zhimg.com/v2-95aad6469883b2244f7888f9447ed47c_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-95aad6469883b2244f7888f9447ed47c_720w.jpg" data-size="normal" data-rawwidth="1200" data-rawheight="500" class="origin_image zh-lightbox-thumb lazy" width="1200" data-original="https://pic1.zhimg.com/v2-95aad6469883b2244f7888f9447ed47c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-95aad6469883b2244f7888f9447ed47c_b.jpg" data-lazy-status="ok"><figcaption>图 8.3初始化（Q,K,V）</figcaption></figure><p class="ztext-empty-paragraph"><br></p><p>通过Linear线性投影来初始化不同的(Q,K,V) ，将多个单头的结果融合会比单头Self-Attention的效果好。我们可以将初始化不同的(Q,K,V) 理解为单头从不同的方向去观察文本，这样使Self-Attention更加具有“大局观”。整体的运算逻辑就是Multi-Head Self-Attention将多个不同单头的Self-Attention输出Concat成一条，然后再经过一个全连接层降维输出，如图 8.4所示。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-dc18cda2c95246a1bb1161a27d3f1411_b.jpg" data-size="normal" data-rawwidth="839" data-rawheight="472" class="origin_image zh-lightbox-thumb" width="839" data-original="https://pic2.zhimg.com/v2-dc18cda2c95246a1bb1161a27d3f1411_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-dc18cda2c95246a1bb1161a27d3f1411_720w.jpg" data-size="normal" data-rawwidth="839" data-rawheight="472" class="origin_image zh-lightbox-thumb lazy" width="839" data-original="https://pic2.zhimg.com/v2-dc18cda2c95246a1bb1161a27d3f1411_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-dc18cda2c95246a1bb1161a27d3f1411_b.jpg" data-lazy-status="ok"><figcaption>图 8.4 Multi-Head Self-Attention（左边为单头Self-Attention运算逻辑，右边为多头Self-Attention运算逻辑）</figcaption></figure><p><b>2. Layer Normalization</b></p><p>Self-Attention的输出会经过Layer Normalization，为什么选择Layer Normalization而不是Batch Normalization？此时，我们应该先对我们的数据形状有个直观的认识，当一个batch的数据输入模型的时候，形状是长方体如图 8.5所示，大小为(batch_size, max_len, embedding)，其中batch_size为batch的批数，max_len为每一批数据的序列最大长度，embedding则为每一个单词或者字的embedding维度大小。而Batch Normalization是对每个Batch的每一列做normalization，相当于是对batch里相同位置的字或者单词embedding做归一化，Layer Normalization是Batch的每一行做normalization，相当于是对每句话的embedding做归一化。显然，LN更加符合我们处理文本的直觉。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-7112b1269ee4c85d9281c2cd268a4bd7_b.jpg" data-size="normal" data-rawwidth="994" data-rawheight="346" class="origin_image zh-lightbox-thumb" width="994" data-original="https://pic4.zhimg.com/v2-7112b1269ee4c85d9281c2cd268a4bd7_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-7112b1269ee4c85d9281c2cd268a4bd7_720w.jpg" data-size="normal" data-rawwidth="994" data-rawheight="346" class="origin_image zh-lightbox-thumb lazy" width="994" data-original="https://pic4.zhimg.com/v2-7112b1269ee4c85d9281c2cd268a4bd7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-7112b1269ee4c85d9281c2cd268a4bd7_b.jpg" data-lazy-status="ok"><figcaption>图 8.5 Layer Normalization与Batch Normalization</figcaption></figure><p><b>3. BERT预训练</b></p><p>如图 8.6所示。</p><p>1) 预训练过程是生成BERT模型的过程</p><p>一般来说，个人用不着自己训练一个BERT预训练模型，都是直接调用模型的权重，进行fine-tune以适应当前特定任务，但我们可以了解一下BERT是怎么训练出来的。</p><p>2) 输入X</p><p>就是Self-Attention Layer的输入，利用字典将每一个字或者单词用数字表示，并转换成token embedding + segment embedding + position embedding。序列的长度一般有512 或者 1024，不足用“[PAD]”补充。句子开头第一个位置用“[CLS]”表示，如果是输入两句话，则用“[SEP]”隔开。</p><p>3) MaskLM策略</p><p>对于输入X，15%的字或者英文单词采用随机掩盖策略。对于这15%的字或者英文单词，80%的概率用“[mask]”替换序列中的某个字或者英文单词，10%的概率替换序列中的某个字或者英文单词，10%的概率不做任何变换。</p><p>4) 训练语料总量</p><p>330亿语料</p><p>5) 预训练</p><p>两种训练同时进行:</p><p>(1) 预测被掩盖的字或者英文单词（MaskLM）。</p><p>(2) 预测两句话之间是否有顺序关系（Next Sentence Prediction）。</p><p>这里需要补充说明的是NLP的预训练模型与计算机视觉的预训练模型有些许不同，NLP的预训练方式采用的是无监督学习，即我们不需要人工打标签，而计算机视觉需要则需要对图像进行人工分类。因为NLP的预训练正如笔者所说，只是预测被掩盖的单词或者字，以及判断是两段话是否有顺序关系，这些只需要写个小程序就可以轻松得到相应的标签，无需人工进行大量的标记。</p><p>6) BERT模型权重</p><p>最后经过大量语料的无监督学习，我们得到了BERT预训练模型，BERT自带字典vocab.txt的每一个字或者单词都被768维度的embedding（即权重）所表示。当我们需要完成特定任务时，若对它们的embedding进行微调（即fine-tune），还能更好得适应任务。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-612e9cd1f7d2e3511adc8055d73f8ccd_b.jpg" data-size="normal" data-rawwidth="1200" data-rawheight="512" class="origin_image zh-lightbox-thumb" width="1200" data-original="https://pic2.zhimg.com/v2-612e9cd1f7d2e3511adc8055d73f8ccd_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-612e9cd1f7d2e3511adc8055d73f8ccd_720w.jpg" data-size="normal" data-rawwidth="1200" data-rawheight="512" class="origin_image zh-lightbox-thumb lazy" width="1200" data-original="https://pic2.zhimg.com/v2-612e9cd1f7d2e3511adc8055d73f8ccd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-612e9cd1f7d2e3511adc8055d73f8ccd_b.jpg" data-lazy-status="ok"><figcaption>图8.6 预训练与Fine-Tune过程</figcaption></figure><p><b>4. BERT的 fine-tune过程</b></p><p>如图 8.6所示。</p><p>可以选择是否fine-tune（微调），如果不选择fine-tune，那就是简单地使用BERT的权重，把它完全当成文本特征提取器使用；若使用fine-tune，则相当于在训练过程中微调BERT的权重，以适应我们当前的任务。</p><p>文章提及到如果选择下面这几个参数进行fine-tune调参，任务的完成度会比较好。</p><p>1) Batch Size:16 or 32;</p><p>2) Learning Rate: 5e-5, 3e-5, 2e-5;</p><p>3) Epochs:2, 3, 4;</p><p><b>参考文献</b></p><p>[1] Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.</p><p>[2] Devlin J, Chang M-W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</p><p>[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]. Advances in neural information processing systems, 2017: 5998-6008.</p><p class="ztext-empty-paragraph"><br></p><p>下一期，我们将继续接受其他预训练模型</p><a target="_blank" href="https://zhuanlan.zhihu.com/p/101610592" data-draft-node="block" data-draft-type="link-card" data-image="https://pic3.zhimg.com/v2-98d0e77b2020be708914192cf082a166_180x120.jpg" data-image-width="1181" data-image-height="656" class="LinkCard LinkCard--hasImage" data-za-detail-view-id="172"><span class="LinkCard-backdrop" style="background-image:url(https://pic3.zhimg.com/v2-98d0e77b2020be708914192cf082a166_180x120.jpg)"></span><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">阿力阿哩哩：BERT与其他预训练模型</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"></path></svg></span>zhuanlan.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="./BERT的原理与应用 - 知乎_files/v2-98d0e77b2020be708914192cf082a166_180x120.jpg"></span></span></a><p>敬请期待~</p><p>关注我的微信公众号【<b>阿力阿哩哩的炼丹日常</b>】~不定期更新相关专业知识~</p><p>喜欢就<b>点个赞</b>吧~</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-c4b3ca216c48b9d11d204061f97aa12d_b.jpg" data-caption="" data-size="normal" data-rawwidth="1937" data-rawheight="791" class="origin_image zh-lightbox-thumb" width="1937" data-original="https://pic2.zhimg.com/v2-c4b3ca216c48b9d11d204061f97aa12d_r.jpg"/></noscript><img src="./BERT的原理与应用 - 知乎_files/v2-c4b3ca216c48b9d11d204061f97aa12d_720w.jpg" data-caption="" data-size="normal" data-rawwidth="1937" data-rawheight="791" class="origin_image zh-lightbox-thumb lazy" width="1937" data-original="https://pic2.zhimg.com/v2-c4b3ca216c48b9d11d204061f97aa12d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c4b3ca216c48b9d11d204061f97aa12d_b.jpg" data-lazy-status="ok"></figure><p></p></div></div><div class="ContentItem-time">编辑于 02-26</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20743626&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20743626" target="_blank"><div class="Popover"><div id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content">BERT</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19560026&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19560026" target="_blank"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content">自然语言处理</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20746363&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20746363" target="_blank"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content">Transformer</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 335px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;101570806&quot;}}}"><span><button aria-label="赞同 33 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 33</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>5 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="Post-SideActions" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 34">赞同 33</div></div></button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover19-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover19-content"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div><div class="Sticky--holder" style="position: static; inset: auto auto 0px 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://www.zhihu.com/column/c_1173558548797038592"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./BERT的原理与应用 - 知乎_files/v2-63ca9d033cda61f56532d8ffd0cacfe7_xs.jpg" srcset="https://pic4.zhimg.com/v2-63ca9d033cda61f56532d8ffd0cacfe7_l.jpg?source=172ae18b 2x" alt="通俗易懂NLP"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://www.zhihu.com/column/c_1173558548797038592"><div class="Popover"><div id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content">通俗易懂NLP</div></div></a></h2><div class="ContentItem-meta">NLPer的学习路线</div></div><div class="ContentItem-extra"><a href="https://www.zhihu.com/column/c_1173558548797038592" type="button" class="Button">进入专栏</a></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1360px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled="" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/108031414" class="PostItem"><div><img src="./BERT的原理与应用 - 知乎_files/v2-d0e54144cd0a1fd1c34760dc04ac0176_250x0.jpg" srcset="https://pic4.zhimg.com/v2-d0e54144cd0a1fd1c34760dc04ac0176_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="从Transformer到BERT模型"><h1 class="PostItem-Title">从Transformer到BERT模型</h1><div class="PostItem-Footer"><span>Micro...</span><span class="PostItem-FooterTitle">发表于人工智能</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/104501321" class="PostItem"><div><img src="./BERT的原理与应用 - 知乎_files/v2-107ed73d80c04120e4922ef9ce095f73_250x0.jpg" srcset="https://pic1.zhimg.com/v2-107ed73d80c04120e4922ef9ce095f73_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="【NLP】改变NLP格局的利器-BERT(模型和代码解析)"><h1 class="PostItem-Title">【NLP】改变NLP格局的利器-BERT(模型和代码解析)</h1><div class="PostItem-Footer"><span>南枫</span><span class="PostItem-FooterTitle">发表于机器学习实...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/266364526" class="PostItem"><div><h1 class="PostItem-Title">图解 BERT</h1><p class="PostItem-Summary">本文翻译自：http://jalammar.github.io/illustrated-bert/。 通俗易懂，非常适合刚刚开始了解 Bert 的同学。 BERT 来源于 Transformer，如果你不知道 Transformer 是什么，你可以查看 图解…</p><div class="PostItem-Footer"><span>张贤同学</span><span class="PostItem-FooterTitle">发表于技术小窝</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/141527015" class="PostItem"><div><img src="./BERT的原理与应用 - 知乎_files/v2-2856cff8d8cdc841cffbc8ff76ae4950_250x0.jpg" srcset="https://pic1.zhimg.com/v2-2856cff8d8cdc841cffbc8ff76ae4950_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="【HugBert01】Huggingface Transformers，一个顶级自然语言处理框架"><h1 class="PostItem-Title">【HugBert01】Huggingface Transformers，一个顶级自然语言处理框架</h1><div class="PostItem-Footer"><span>套牌神仙</span><span class="PostItem-FooterTitle">发表于人机合一</span></div></div></a><button class="PagingButton PagingButton-Next" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{}"><div class="CommentsV2 CommentsV2--withEditor CommentsV2-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">5 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div><div class="CommentsV2-footer CommentEditorV2--normal"><div class="CommentEditorV2-inputWrap"><div class="InputLike CommentEditorV2-input Editable"><div class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-960bv" style="white-space: pre-wrap;">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-960bv" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="960bv" data-offset-key="b04dq-0-0"><div data-offset-key="b04dq-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="b04dq-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/webp,image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div><div class="CommentEditorV2-inputUpload"><div class="CommentEditorV2-popoverWrap"><div class="Popover CommentEditorV2-inputUpLoad-Icon"><button aria-label="插入表情" data-tooltip="插入表情" data-tooltip-position="bottom" data-tooltip-will-hide-on-click="true" id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content" type="button" class="Button Editable-control Button--plain"><svg class="Zi Zi--Emotion" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M7.523 13.5h8.954c-.228 2.47-2.145 4-4.477 4-2.332 0-4.25-1.53-4.477-4zM12 21a9 9 0 1 1 0-18 9 9 0 0 1 0 18zm0-1.5a7.5 7.5 0 1 0 0-15 7.5 7.5 0 0 0 0 15zm-3-8a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm6 0a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3z"></path></svg></button></div></div></div></div><button type="button" disabled="" class="Button CommentEditorV2-singleButton Button--primary Button--blue">发布</button></div></div><div><div class="CommentListV2"><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/wuxianchao"><img class="Avatar UserLink-avatar" width="24" height="24" src="./BERT的原理与应用 - 知乎_files/v2-a0b700190d6d0215c716cfb56d34ae5f_s.jpg" srcset="https://pic3.zhimg.com/v2-a0b700190d6d0215c716cfb56d34ae5f_xs.jpg?source=06d4cd63 2x" alt="迷途小书僮"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/wuxianchao">迷途小书僮</a></span><span class="CommentItemV2-time">03-29</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>关于Layer Norm和Batch Norm的区分，很赞！</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>2</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover14-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover14-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/huang-zheng-yang-99"><img class="Avatar UserLink-avatar" width="24" height="24" src="./BERT的原理与应用 - 知乎_files/da8e974dc_s.jpg" srcset="https://pic3.zhimg.com/da8e974dc_xs.jpg?source=06d4cd63 2x" alt="人生输家"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/huang-zheng-yang-99">人生输家</a></span><span class="CommentItemV2-time">05-02</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">讲的太好了，看完你的文章茅塞顿开</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootComment"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tan-lu-50-39"><img class="Avatar UserLink-avatar" width="24" height="24" src="./BERT的原理与应用 - 知乎_files/v2-240618ce4f0d1320448b9a11bfc46d49_s.jpg" srcset="https://pic1.zhimg.com/v2-240618ce4f0d1320448b9a11bfc46d49_xs.jpg?source=06d4cd63 2x" alt="MissCatty"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tan-lu-50-39">MissCatty</a></span><span class="CommentItemV2-time">07-01</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>文章很好，赞赞赞！<br>关于Self-attention那一部分里面有点疑问，单词转化为token embedding + segment embedding + position embedding是这三个embedding之和吗？还是说单词转化为了三个embeddings</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover16-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover16-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/qiu-nian-39"><img class="Avatar UserLink-avatar" width="24" height="24" src="./BERT的原理与应用 - 知乎_files/v2-8af8cc2f5be2f64b6a65f28d91dabdb5_s.jpg" srcset="https://pic4.zhimg.com/v2-8af8cc2f5be2f64b6a65f28d91dabdb5_xs.jpg?source=06d4cd63 2x" alt="游走人间的小乌龟"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/qiu-nian-39">游走人间的小乌龟</a></span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tan-lu-50-39">MissCatty</a></span><span class="CommentItemV2-time">08-14</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>每个单词的输入直接转换为了这三个的embedding和。</p></div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>1</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li><li class="NestComment--child"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover17-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover17-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/longl-55"><img class="Avatar UserLink-avatar" width="24" height="24" src="./BERT的原理与应用 - 知乎_files/v2-5ff0bee0a4bca9d6d22769e13644c619_s.jpg" srcset="https://pic4.zhimg.com/v2-5ff0bee0a4bca9d6d22769e13644c619_xs.jpg?source=06d4cd63 2x" alt="知足"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/longl-55">知足</a></span><span class="CommentItemV2-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/tan-lu-50-39">MissCatty</a></span><span class="CommentItemV2-time">12-02</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">之和</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul></div></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" aria-label="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div>

<script id="" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{},"admins":{"data":[]},"members":{"data":[]},"explore":{"candidateSyncClubs":{}},"profile":{},"checkin":{},"comments":{"paging":{},"loading":{},"meta":{},"ids":{}},"postList":{"paging":{},"loading":{},"ids":{}},"recommend":{"data":[]},"silences":{"data":[]},"application":{"profile":null}},"entities":{"users":{"bie-ying-xiang-zhi-li":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3.jpg?source=172ae18b","uid":"819694209459159040","userType":"people","isFollowing":false,"urlToken":"bie-ying-xiang-zhi-li","id":"ac07f316510c6610194a24aa9135538f","description":"半小时","name":"阿力阿哩哩","isAdvertiser":false,"headline":"NLPer，微信公众号同名【阿力阿哩哩】","gender":0,"url":"\u002Fpeople\u002Fac07f316510c6610194a24aa9135538f","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b","isOrg":false,"type":"people","levelInfo":{"exp":53396,"level":6,"nicknameColor":{"color":"","nightModeColor":""},"levelIcon":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","iconInfo":{"url":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","nightModeUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-92beb29b03851affbd2e8114805460cb_l.png","width":93,"height":51}},"badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_l.png?source=172ae18b","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"101570806":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"id":101570806,"title":"BERT的原理与应用","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F101570806","imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-88b281667a417305b70bcc826a831f9c_720w.jpg?source=172ae18b","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-88b281667a417305b70bcc826a831f9c_720w.jpg?source=172ae18b","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f01f69b5693df76cebd8aec73c8ec4c_200x112.png\" data-caption=\"图 8.1 Transformers（左边为Encoder，右边为Decoder）\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"785\" data-watermark=\"original\" data-original-src=\"v2-6f01f69b5693df76cebd8aec73c8ec4c\" data-watermark-src=\"v2-30b3e88c18b4a484268da3c7e64be035\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f01f69b5693df76cebd8aec73c8ec4c_r.png\"\u002F\u003ENLP目录\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F107833571\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4b3ca216c48b9d11d204061f97aa12d_180x120.jpg\" data-image-width=\"1937\" data-image-height=\"791\" class=\"internal\"\u003E阿力阿哩哩：NLP目录\u003C\u002Fa\u003E我们在第七章介绍了迁移学习与计算机视觉的故事，不过好故事并没有这么快结束。迁移学习一路前行，走进了自然语言处理的片场。迁移学习在自然语言处理（NLP）领域同样也是一种强大的技术。由这种技术训练出来的模型，我们称之为…","created":1578413192,"updated":1582687208,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3.jpg?source=172ae18b","uid":"819694209459159040","userType":"people","isFollowing":false,"urlToken":"bie-ying-xiang-zhi-li","id":"ac07f316510c6610194a24aa9135538f","description":"半小时","name":"阿力阿哩哩","isAdvertiser":false,"headline":"NLPer，微信公众号同名【阿力阿哩哩】","gender":0,"url":"\u002Fpeople\u002Fac07f316510c6610194a24aa9135538f","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b","isOrg":false,"type":"people","levelInfo":{"exp":53396,"level":6,"nicknameColor":{"color":"","nightModeColor":""},"levelIcon":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","iconInfo":{"url":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","nightModeUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-92beb29b03851affbd2e8114805460cb_l.png","width":93,"height":51}},"badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_l.png?source=172ae18b","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","copyrightPermission":"need_review","state":"published","imageWidth":1255,"imageHeight":536,"content":"\u003Ch2\u003ENLP目录\u003C\u002Fh2\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F107833571\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4b3ca216c48b9d11d204061f97aa12d_180x120.jpg\" data-image-width=\"1937\" data-image-height=\"791\" class=\"internal\"\u003E阿力阿哩哩：NLP目录\u003C\u002Fa\u003E\u003Cp\u003E我们在第七章介绍了迁移学习与计算机视觉的故事，不过好故事并没有这么快结束。迁移学习一路前行，走进了自然语言处理的片场。迁移学习在自然语言处理（NLP）领域同样也是一种强大的技术。由这种技术训练出来的模型，我们称之为预训练模型。\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E预训练模型首先要针对数据丰富的任务进行预训练，然后再针对下游任务进行微调，以达到下游任务的最佳效果。迁移学习的有效性引起了理论和实践的多样性，人们通过迁移学习与自然语言处理两者相结合，高效地完成了各种NLP的实际任务。\u003C\u002Fp\u003E\u003Ch2\u003E8.1 自然语言处理预训练模型\u003C\u002Fh2\u003E\u003Cp\u003E使语言建模和其他学习问题变得困难的一个基本问题是维数的诅咒。在人们想要对许多离散的随机变量（例如句子中的单词或数据挖掘任务中的离散属性）之间的联合分布建模时，这一点尤其明显。\u003C\u002Fp\u003E\u003Cp\u003E举个例子，假如我们有10000个单词的词汇表，我们要对它们进行离散表示，这样用one-hot 编码整个词汇表就需要10000*10000的矩阵，而one-hot编码矩阵存在很多“0”值，显然浪费了绝大部分的内存空间。为了解决维度诅咒带来的问题，人们开始使用低维度的向量空间来表示单词，从而减少运算资源的损耗，这也是预训练模型思想的开端。\u003C\u002Fp\u003E\u003Ch2\u003E8.1.1 Word2Vec\u003C\u002Fh2\u003E\u003Cp\u003E在4.7.2节的实验中{\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F106961648\" class=\"internal\"\u003EWord2Vec｜Skip-gram\u003C\u002Fa\u003E}，我们提及了Skip-gram模型，它就是Yoshua Bengio等人[1]提出的经典Word2Vec模型之一。Word2Vec模型对NLP任务的效果有显著的提升，并且能够利用更长的上下文。对于Word2Vec具体的原理与应用，笔者在4.7.2节已经进行了详细的讲解，而且本章的主角并不是它，所以笔者就不进行赘述了。\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F106961648\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a2e792d91abbc2dc2b8b68690b287d0f_180x120.jpg\" data-image-width=\"720\" data-image-height=\"388\" class=\"internal\"\u003E阿力阿哩哩：Word2Vec｜Skip-gram\u003C\u002Fa\u003E\u003Ch2\u003E8.1.2 BERT\u003C\u002Fh2\u003E\u003Cp\u003E在2018年，什么震惊了NLP学术界？毫无疑问是Jacob Devlin等人[2]提出的预训练模型BERT（Bidirectional Encoder Representations from Transformers）。\u003C\u002Fp\u003E\u003Cp\u003EBERT被设计为通过在所有层的双向上下文上共同进行条件化来预训练未标记文本的深层双向表示。我们可以在仅一个附加输出层的情况下对经过预训练的BERT模型进行微调，以创建适用于各种任务（例如问题解答和语言推断）的最新模型，进而减少了对NLP任务精心设计特定体系结构的需求。BERT是第一个基于微调的表示模型，可在一系列句子级和字符级任务上实现最先进的性能，优于许多特定于任务的体系结构。\u003C\u002Fp\u003E\u003Cp\u003E通俗易懂来讲就是我们只需要把BERT当成一个深层次的Word2Vec预训练模型，对于一些特定的任务，我们只需要在BERT之后下接一些网络结构就可以出色地完成这些任务。\u003C\u002Fp\u003E\u003Cp\u003E另外，2018年底提出的BERT推动了11项NLP任务的发展。BERT的结构是来自Transformers模型的Encoder，Transformers如图 8.1所示。我们从图X中可以看到Transformer的内部结构都是由Ashish Vaswani 等人[3]提出的Self-Attention Layer和Layer Normalization的堆叠而产生。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f01f69b5693df76cebd8aec73c8ec4c_b.jpg\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb\" width=\"537\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f01f69b5693df76cebd8aec73c8ec4c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;537&#39; height=&#39;785&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"537\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f01f69b5693df76cebd8aec73c8ec4c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f01f69b5693df76cebd8aec73c8ec4c_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 8.1 Transformers（左边为Encoder，右边为Decoder）\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E1. Self-Attention Layer原理\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E1) Self-Attention Layer的出现原因\u003C\u002Fp\u003E\u003Cp\u003E为了解决RNN、LSTM等常用于处理序列化数据的网络结构无法在GPU中并行加速计算的问题。\u003C\u002Fp\u003E\u003Cp\u003E2) Self-Attention Layer的输入\u003C\u002Fp\u003E\u003Cp\u003E如图8.2所示：将输入的Input转化成token embedding + segment embedding +position embedding。因为有时候训练样本是由两句话组成，因此“[CLS]”是用来分类输入的两句话是否有上下文关系，而“[SEP]”则是用以分开两句话的标志符。其中，因为这里的Input是英文单词，所以在灌入模型之前，需要用BERT源码的Tokenization工具对每一个单词进行分词，分词后的形式如图 8.2中Input的“Playing”转换成“Play”+“# #ing”，因为英文词汇表是通过词根与词缀的组合来新增单词语义的，所以我们选择用分词方法可以减少整体的词汇表长度。如果是中文字符的话，输入就不需要分词，整段话的每一个字用“空格”隔开即可。\u003C\u002Fp\u003E\u003Cp\u003E值得注意的是，模型是无法处理文本字符的，所以不管是英文还是中文，我们都需要通过预训练模型BERT自带的字典vocab.txt将每一个字或者单词转换成字典索引（即id）输入。\u003C\u002Fp\u003E\u003Cp\u003E(1) segment embedding的目的：有些任务是两句话一起放入输入X，而segment便是用来区分这两句话的。在Input那里就是用“[SEP]”作为标志符号。而“[CLS]”用来分类输入的两句话是否有上下文关系。\u003C\u002Fp\u003E\u003Cp\u003E(2) position embedding的目的：因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding。构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding：使用正弦函数，位置维度对应曲线，而且方便序列之间的选对位置，使用正弦会比余弦好的原因是可以在训练过程中，将原本序列外拓成比原来序列还要长的序列，如公式（8.1）~（8.2）所示。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4c4d98fd827d401663cdc0d6ab8fbbd0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"706\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4c4d98fd827d401663cdc0d6ab8fbbd0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;706&#39; height=&#39;148&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"706\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4c4d98fd827d401663cdc0d6ab8fbbd0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4c4d98fd827d401663cdc0d6ab8fbbd0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b097414710b1f094ecc3fe421afd275f_b.jpg\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb\" width=\"984\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b097414710b1f094ecc3fe421afd275f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;984&#39; height=&#39;321&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"984\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b097414710b1f094ecc3fe421afd275f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b097414710b1f094ecc3fe421afd275f_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 8.2 Self-Attention的输入\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E首先，将 Q与K 矩阵乘积并scale（为了防止结果过大，除以他们维度的均方根），其次，将其灌入Softmax函数得到概率分布，最后再与V 矩阵相乘，得到self-attention的输出，如公式（8.3）所示。其中，(Q,K,V) 均来自同一输入X ，他们是 X分别乘上 WQ,WK,WV初始化权值矩阵所得，而后这三个权值矩阵会在训练的过程中确定下来，如图 8.3所示。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f03dbc9f1ef3ad994cf12cbabd0a80ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"89\" class=\"content_image\" width=\"367\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;367&#39; height=&#39;89&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"89\" class=\"content_image lazy\" width=\"367\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f03dbc9f1ef3ad994cf12cbabd0a80ab_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-95aad6469883b2244f7888f9447ed47c_b.jpg\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-95aad6469883b2244f7888f9447ed47c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1200&#39; height=&#39;500&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-95aad6469883b2244f7888f9447ed47c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-95aad6469883b2244f7888f9447ed47c_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 8.3初始化（Q,K,V）\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E通过Linear线性投影来初始化不同的(Q,K,V) ，将多个单头的结果融合会比单头Self-Attention的效果好。我们可以将初始化不同的(Q,K,V) 理解为单头从不同的方向去观察文本，这样使Self-Attention更加具有“大局观”。整体的运算逻辑就是Multi-Head Self-Attention将多个不同单头的Self-Attention输出Concat成一条，然后再经过一个全连接层降维输出，如图 8.4所示。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc18cda2c95246a1bb1161a27d3f1411_b.jpg\" data-size=\"normal\" data-rawwidth=\"839\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"839\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc18cda2c95246a1bb1161a27d3f1411_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;839&#39; height=&#39;472&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"839\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"839\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc18cda2c95246a1bb1161a27d3f1411_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc18cda2c95246a1bb1161a27d3f1411_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 8.4 Multi-Head Self-Attention（左边为单头Self-Attention运算逻辑，右边为多头Self-Attention运算逻辑）\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E2. Layer Normalization\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003ESelf-Attention的输出会经过Layer Normalization，为什么选择Layer Normalization而不是Batch Normalization？此时，我们应该先对我们的数据形状有个直观的认识，当一个batch的数据输入模型的时候，形状是长方体如图 8.5所示，大小为(batch_size, max_len, embedding)，其中batch_size为batch的批数，max_len为每一批数据的序列最大长度，embedding则为每一个单词或者字的embedding维度大小。而Batch Normalization是对每个Batch的每一列做normalization，相当于是对batch里相同位置的字或者单词embedding做归一化，Layer Normalization是Batch的每一行做normalization，相当于是对每句话的embedding做归一化。显然，LN更加符合我们处理文本的直觉。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7112b1269ee4c85d9281c2cd268a4bd7_b.jpg\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb\" width=\"994\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7112b1269ee4c85d9281c2cd268a4bd7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;994&#39; height=&#39;346&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"994\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7112b1269ee4c85d9281c2cd268a4bd7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7112b1269ee4c85d9281c2cd268a4bd7_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图 8.5 Layer Normalization与Batch Normalization\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E3. BERT预训练\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E如图 8.6所示。\u003C\u002Fp\u003E\u003Cp\u003E1) 预训练过程是生成BERT模型的过程\u003C\u002Fp\u003E\u003Cp\u003E一般来说，个人用不着自己训练一个BERT预训练模型，都是直接调用模型的权重，进行fine-tune以适应当前特定任务，但我们可以了解一下BERT是怎么训练出来的。\u003C\u002Fp\u003E\u003Cp\u003E2) 输入X\u003C\u002Fp\u003E\u003Cp\u003E就是Self-Attention Layer的输入，利用字典将每一个字或者单词用数字表示，并转换成token embedding + segment embedding + position embedding。序列的长度一般有512 或者 1024，不足用“[PAD]”补充。句子开头第一个位置用“[CLS]”表示，如果是输入两句话，则用“[SEP]”隔开。\u003C\u002Fp\u003E\u003Cp\u003E3) MaskLM策略\u003C\u002Fp\u003E\u003Cp\u003E对于输入X，15%的字或者英文单词采用随机掩盖策略。对于这15%的字或者英文单词，80%的概率用“[mask]”替换序列中的某个字或者英文单词，10%的概率替换序列中的某个字或者英文单词，10%的概率不做任何变换。\u003C\u002Fp\u003E\u003Cp\u003E4) 训练语料总量\u003C\u002Fp\u003E\u003Cp\u003E330亿语料\u003C\u002Fp\u003E\u003Cp\u003E5) 预训练\u003C\u002Fp\u003E\u003Cp\u003E两种训练同时进行:\u003C\u002Fp\u003E\u003Cp\u003E(1) 预测被掩盖的字或者英文单词（MaskLM）。\u003C\u002Fp\u003E\u003Cp\u003E(2) 预测两句话之间是否有顺序关系（Next Sentence Prediction）。\u003C\u002Fp\u003E\u003Cp\u003E这里需要补充说明的是NLP的预训练模型与计算机视觉的预训练模型有些许不同，NLP的预训练方式采用的是无监督学习，即我们不需要人工打标签，而计算机视觉需要则需要对图像进行人工分类。因为NLP的预训练正如笔者所说，只是预测被掩盖的单词或者字，以及判断是两段话是否有顺序关系，这些只需要写个小程序就可以轻松得到相应的标签，无需人工进行大量的标记。\u003C\u002Fp\u003E\u003Cp\u003E6) BERT模型权重\u003C\u002Fp\u003E\u003Cp\u003E最后经过大量语料的无监督学习，我们得到了BERT预训练模型，BERT自带字典vocab.txt的每一个字或者单词都被768维度的embedding（即权重）所表示。当我们需要完成特定任务时，若对它们的embedding进行微调（即fine-tune），还能更好得适应任务。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-612e9cd1f7d2e3511adc8055d73f8ccd_b.jpg\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-612e9cd1f7d2e3511adc8055d73f8ccd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1200&#39; height=&#39;512&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-612e9cd1f7d2e3511adc8055d73f8ccd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-612e9cd1f7d2e3511adc8055d73f8ccd_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图8.6 预训练与Fine-Tune过程\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E4. BERT的 fine-tune过程\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E如图 8.6所示。\u003C\u002Fp\u003E\u003Cp\u003E可以选择是否fine-tune（微调），如果不选择fine-tune，那就是简单地使用BERT的权重，把它完全当成文本特征提取器使用；若使用fine-tune，则相当于在训练过程中微调BERT的权重，以适应我们当前的任务。\u003C\u002Fp\u003E\u003Cp\u003E文章提及到如果选择下面这几个参数进行fine-tune调参，任务的完成度会比较好。\u003C\u002Fp\u003E\u003Cp\u003E1) Batch Size:16 or 32;\u003C\u002Fp\u003E\u003Cp\u003E2) Learning Rate: 5e-5, 3e-5, 2e-5;\u003C\u002Fp\u003E\u003Cp\u003E3) Epochs:2, 3, 4;\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E参考文献\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E[1] Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\u003C\u002Fp\u003E\u003Cp\u003E[2] Devlin J, Chang M-W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\u003C\u002Fp\u003E\u003Cp\u003E[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]. Advances in neural information processing systems, 2017: 5998-6008.\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E下一期，我们将继续接受其他预训练模型\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F101610592\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98d0e77b2020be708914192cf082a166_180x120.jpg\" data-image-width=\"1181\" data-image-height=\"656\" class=\"internal\"\u003E阿力阿哩哩：BERT与其他预训练模型\u003C\u002Fa\u003E\u003Cp\u003E敬请期待~\u003C\u002Fp\u003E\u003Cp\u003E关注我的微信公众号【\u003Cb\u003E阿力阿哩哩的炼丹日常\u003C\u002Fb\u003E】~不定期更新相关专业知识~\u003C\u002Fp\u003E\u003Cp\u003E喜欢就\u003Cb\u003E点个赞\u003C\u002Fb\u003E吧~\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4b3ca216c48b9d11d204061f97aa12d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1937\" data-rawheight=\"791\" class=\"origin_image zh-lightbox-thumb\" width=\"1937\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4b3ca216c48b9d11d204061f97aa12d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1937&#39; height=&#39;791&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1937\" data-rawheight=\"791\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1937\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4b3ca216c48b9d11d204061f97aa12d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4b3ca216c48b9d11d204061f97aa12d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20743626","type":"topic","id":"20743626","name":"BERT"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19560026","type":"topic","id":"19560026","name":"自然语言处理"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20746363","type":"topic","id":"20746363","name":"Transformer"}],"voteupCount":33,"voting":0,"column":{"description":"本专栏旨在通俗易懂的方式来介绍近年来流行的NLP模型与实验。","canManage":false,"intro":"NLPer的学习路线","isFollowing":false,"urlToken":"c_1173558548797038592","id":"c_1173558548797038592","articlesCount":21,"acceptSubmission":true,"title":"通俗易懂NLP","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1173558548797038592","commentPermission":"all","created":1572578161,"updated":1599167451,"imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-63ca9d033cda61f56532d8ffd0cacfe7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3.jpg?source=172ae18b","uid":"819694209459159040","userType":"people","isFollowing":false,"urlToken":"bie-ying-xiang-zhi-li","id":"ac07f316510c6610194a24aa9135538f","description":"半小时","name":"阿力阿哩哩","isAdvertiser":false,"headline":"NLPer，微信公众号同名【阿力阿哩哩】","gender":0,"url":"\u002Fpeople\u002Fac07f316510c6610194a24aa9135538f","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":219,"type":"column"},"commentCount":5,"contributions":[{"id":22747657,"state":"accepted","type":"first_publish","column":{"description":"本专栏旨在通俗易懂的方式来介绍近年来流行的NLP模型与实验。","canManage":false,"intro":"NLPer的学习路线","isFollowing":false,"urlToken":"c_1173558548797038592","id":"c_1173558548797038592","articlesCount":21,"acceptSubmission":true,"title":"通俗易懂NLP","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1173558548797038592","commentPermission":"all","created":1572578161,"updated":1599167451,"imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-63ca9d033cda61f56532d8ffd0cacfe7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3.jpg?source=172ae18b","uid":"819694209459159040","userType":"people","isFollowing":false,"urlToken":"bie-ying-xiang-zhi-li","id":"ac07f316510c6610194a24aa9135538f","description":"半小时","name":"阿力阿哩哩","isAdvertiser":false,"headline":"NLPer，微信公众号同名【阿力阿哩哩】","gender":0,"url":"\u002Fpeople\u002Fac07f316510c6610194a24aa9135538f","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":219,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"favlistsCount":75,"isNormal":true,"status":0,"shareText":"BERT的原理与应用 - 来自知乎专栏「通俗易懂NLP」，作者: 阿力阿哩哩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F101570806 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":6,"visibleOnlyToAuthor":false,"hasColumn":true,"republishers":[]}},"columns":{"c_1173558548797038592":{"description":"本专栏旨在通俗易懂的方式来介绍近年来流行的NLP模型与实验。","canManage":false,"intro":"NLPer的学习路线","isFollowing":false,"urlToken":"c_1173558548797038592","id":"c_1173558548797038592","articlesCount":21,"acceptSubmission":true,"title":"通俗易懂NLP","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1173558548797038592","commentPermission":"all","created":1572578161,"updated":1599167451,"imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-63ca9d033cda61f56532d8ffd0cacfe7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3.jpg?source=172ae18b","uid":"819694209459159040","userType":"people","isFollowing":false,"urlToken":"bie-ying-xiang-zhi-li","id":"ac07f316510c6610194a24aa9135538f","description":"半小时","name":"阿力阿哩哩","isAdvertiser":false,"headline":"NLPer，微信公众号同名【阿力阿哩哩】","gender":0,"url":"\u002Fpeople\u002Fac07f316510c6610194a24aa9135538f","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf1176d84800efad9719a57d03ca92d3_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":219,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-qa_cl_guest-2","expPrefix":"qa_cl_guest","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_item-3","expPrefix":"se_item","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_timeguide-2","expPrefix":"vd_timeguide","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_video_replay-3","expPrefix":"vd_video_replay","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_bullet_gui-4","expPrefix":"vd_bullet_gui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_vserial-7","expPrefix":"vd_vserial","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"web_nvgt-2_v8","expPrefix":"web_nvgt","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"Test_Punk-1_v2","expPrefix":"Test_Punk","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"meta_ebook-2_v1","expPrefix":"meta_ebook","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"zanswer-1_v8","expPrefix":"zanswer","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"recnew_2th-1_v3","expPrefix":"recnew_2th","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"club_fn-1_v4","expPrefix":"club_fn","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"ge_newbie3-3_v1","expPrefix":"ge_newbie3","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_sug_topic-1_v1","expPrefix":"se_sug_topic","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"correct_pos-4_v2","expPrefix":"correct_pos","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"correct_gpu-2_v3","expPrefix":"correct_gpu","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_cvr_boost-3_v1","expPrefix":"se_cvr_boost","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"general_1-2_v1","expPrefix":"general_1","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"ge_sug_rep","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1034","key":3158},{"id":"hot_card","type":"Int","value":"0","chainId":"_gene_","layerId":"hot_card","key":108},{"id":"se_cvr_boost","type":"Int","value":"1","chainId":"_gene_","layerId":"se_cvr_boost","key":183},{"id":"web_login","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_11"},{"id":"ge_newyanzhi","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_1019","key":2788},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_","layerId":"iosus_layer_1"},{"id":"gue_sharp","type":"String","value":"1","layerId":"guevd_layer_686"},{"id":"show_ad","type":"Int","value":"0","chainId":"_gene_","layerId":"show_ad","key":27},{"id":"ge_prf_rec","type":"String","value":"0","chainId":"_gene_","layerId":"getop_layer_991","key":3040},{"id":"se_sug_topic","type":"Int","value":"0","chainId":"_gene_","layerId":"se_sug_topic","key":230},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_7"},{"id":"captcha_v2","type":"Int","value":"0","layerId":"captcha_v2"},{"id":"se_tb_rank","type":"Int","value":"0","chainId":"_gene_","layerId":"se_tb_rank","key":194},{"id":"ge_v071","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_v071","key":224},{"id":"gue_profile_video","type":"String","value":"1","layerId":"guevd_layer_5"},{"id":"gue_bulletmb","type":"String","value":"0","layerId":"guevd_layer_812"},{"id":"club_fn","type":"Int","value":"1","layerId":"club_fn"},{"id":"ge_v070","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_v070","key":173},{"id":"ge_flow_join","type":"String","value":"1","chainId":"_gene_","layerId":"getp_layer_872","key":2988},{"id":"web_creator_route","type":"String","value":"1","layerId":"webtop_layer_1"},{"id":"pfd_newbie2","type":"Int","value":"0","chainId":"_gene_","layerId":"pfd_newbie2","key":71},{"id":"li_panswer_topic","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_602"},{"id":"gue_card_test","type":"String","value":"1","layerId":"gueqa_layer_2"},{"id":"ge_item","type":"String","value":"2","chainId":"_gene_","layerId":"gese_layer_945","key":2971},{"id":"ge_kocbox","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_945","key":3087},{"id":"tp_zrec","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_619"},{"id":"ge_upload","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_839","key":2892},{"id":"ge_video","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":2831},{"id":"gue_fo_recom","type":"String","value":"0","layerId":"gueqa_layer_780"},{"id":"ge_sug_v2","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3189},{"id":"ge_usercard1","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":2740},{"id":"gue_bullet_second","type":"String","value":"1","layerId":"guevd_layer_1"},{"id":"Test_Punk","type":"Int","value":"0","layerId":"Test_Punk"},{"id":"tp_dingyue_video","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"gue_bullet_guide","type":"String","value":"点我，做第一个上屏的人","layerId":"guevd_layer_0"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"gue_art_ui","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"se_no_rwrite","type":"Int","value":"0","chainId":"_gene_","layerId":"se_no_rwrite","key":164},{"id":"web_column_auto_invite","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"web_audit_01","type":"String","value":"case1","layerId":"webre_layer_1"},{"id":"pfd_newbie","type":"Int","value":"0","chainId":"_gene_","layerId":"pfd_newbie","key":63},{"id":"correct_gpu","type":"Int","value":"1","chainId":"_gene_","layerId":"correct_gpu","key":66},{"id":"web_heifetz_grow_ad","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"li_sp_mqbk","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_748"},{"id":"ge_entity","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_946","key":3036},{"id":"gue_vid_tab","type":"String","value":"0","layerId":"guevd_layer_900"},{"id":"general_1","type":"Int","value":"2","chainId":"_gene_","layerId":"general_1","key":8},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_718"},{"id":"gue_self_censoring","type":"String","value":"1","layerId":"gueqa_layer_1"},{"id":"web_answer_list_ad","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"se_fix_ebook","type":"Int","value":"0","chainId":"_gene_","layerId":"se_fix_ebook","key":103},{"id":"ge_v068","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_v068","key":139},{"id":"ge_yuzhi_v1","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1029","key":3127},{"id":"gue_video_replay","type":"String","value":"2","layerId":"guevd_layer_3"},{"id":"web_nvgt","type":"Int","value":"1","layerId":"web_nvgt"},{"id":"ge_meta_ss","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_834","key":3079},{"id":"tp_contents","type":"String","value":"2","chainId":"_all_","layerId":"tptp_layer_627"},{"id":"gue_video_guide","type":"String","value":"1","layerId":"guevd_layer_625"},{"id":"web_scl_rec","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"ge_base_only","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1029","key":3172},{"id":"meta_ebook","type":"Int","value":"1","layerId":"meta_ebook"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_2"},{"id":"gue_goods_card","type":"String","value":"0","layerId":"gueqa_layer_1"},{"id":"ge_sxzx","type":"String","value":"0","chainId":"_gene_","layerId":"gere_layer_990","key":3060},{"id":"web_unfriendly_comm","type":"String","value":"0","layerId":"webre_layer_1"},{"id":"ge_hard_s_ma","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":3031},{"id":"gue_q_share","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"ge_search_ui","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_838","key":2898},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_","layerId":"sese_layer_4"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_5"},{"id":"ge_recall","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1029","key":3110},{"id":"ge_relation2","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_815","key":2796},{"id":"correct_pos","type":"Int","value":"2","chainId":"_gene_","layerId":"correct_pos","key":104},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_3"},{"id":"web_collection_guest","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"ge_emoji","type":"String","value":"0","chainId":"_gene_","layerId":"getp_layer_827","key":3209},{"id":"ge_newbie3","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_newbie3","key":180},{"id":"ge_rec_2th","type":"String","value":"11","chainId":"_gene_","layerId":"geli_layer_965","key":3023},{"id":"gue_art2qa","type":"String","value":"0","layerId":"gueqa_layer_579"},{"id":"gue_messrec","type":"String","value":"0","layerId":"gueqa_layer_769"},{"id":"ge_infinity6","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_815","key":2817},{"id":"ge_guess","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_938","key":2912},{"id":"gue_recmess","type":"String","value":"0","layerId":"gueqa_layer_795"},{"id":"gue_playh_an","type":"String","value":"0","layerId":"guevd_layer_622"},{"id":"ge_rec_sup","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_rec_sup","key":197},{"id":"zanswer","type":"Int","value":"0","layerId":"zanswer"},{"id":"ge_aaaa","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_aaaa","key":121},{"id":"gue_repost","type":"String","value":"0","layerId":"gueqa_layer_671"},{"id":"ge_v_rank_v3","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1047","key":2966},{"id":"ge_corr","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_976","key":3041},{"id":"gue_iosplay","type":"String","value":"0","layerId":"guevd_layer_896"},{"id":"recnew_2th","type":"Int","value":"24","layerId":"recnew_2th"},{"id":"gue_visit_n_artcard","type":"String","value":"1","layerId":"gueqa_layer_579"},{"id":"gue_v_serial","type":"String","value":"4","layerId":"guevd_layer_695"},{"id":"web_sem_ab","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"ge_newcard","type":"String","value":"3","chainId":"_gene_","layerId":"geus_layer_839","key":2997},{"id":"web_ad_banner","type":"String","value":"0","layerId":"webgw_layer_3"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_9"},{"id":"gue_zvideo_link","type":"String","value":"1","layerId":"guevd_layer_2"},{"id":"gue_andplayd","type":"String","value":"0","layerId":"guevd_layer_686"},{"id":"li_edu_page","type":"String","value":"old","chainId":"_all_","layerId":"lili_layer_580"},{"id":"gue_cdzixun","type":"String","value":"0","layerId":"gueqa_layer_3"},{"id":"web_answerlist_ad","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"ge_dipin_pre","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3124}],"chains":[{"chainId":"_all_"}],"encodedParams":"ClpWDGwAtwDkChsA4AvmAMIA4ACtAKwLRwCbCw8MTAsPC3UMtAqkAD8AQgDcCwgAZwCLADcMBwxkDPQL1wtSCyYM7ApoAIkMtADPCwELYAvFAHkAlgvhC7ULNAwSLQEAAQAAAAAAAAABAAIAAAAAAAAAAQACAAABAAAAAAEAAQIAAAsAAAAAAAEDAA=="},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F87.0.4280.88 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F101570806","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F101570806","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":false,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"baiduSearch":false,"googleSearch":true,"miniProgram":false,"xiaomi":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fwww.google.com\u002F","xUDID":"ADCY4SqIQxKPTkxGpvX8DZsM7d4R_otlQcI=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"深圳","countryName":"中国","regionName":"广东","countryCode":"CN"},"logged":false,"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{},"videoSupport":{},"mcnManage":{},"tasks":{},"recentlyCreated":[]},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"room":{"meta":{},"isFetching":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"c_1173558548797038592"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0},"visible":false}},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"zvideos":{"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"club":{}},"republish":{}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="./BERT的原理与应用 - 知乎_files/vendor.546efe75409c9aff627d.js.下載"></script><script src="./BERT的原理与应用 - 知乎_files/column.app.ef7d0388075407ff11f7.js.下載"></script><script src="./BERT的原理与应用 - 知乎_files/hm.js.下載" async=""></script><script src="./BERT的原理与应用 - 知乎_files/zap.js.下載"></script><script src="./BERT的原理与应用 - 知乎_files/push.js.下載"></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><div><div><div class="css-8pdeid"></div></div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete12-0" id="Popover11-toggle" aria-haspopup="true" aria-owns="Popover11-content" class="Input" placeholder="选择语言" value=""><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></label></div></div></div></div></div></body></html>