# CNN Introduce-捲積神經網路介紹



## 大綱

### 1.捲積神經網路介紹

### 2.捲積

### 3.池化

### 4.Alexnet 模型

---

## 1.捲積神經網路介紹

卷積神經網路（**C**onvoluational **N**eural **N**etwork, 後簡稱為 CNN)是一種神經網路架構，近年在人臉辨識、圖像分類、自動駕駛等領域大放異彩。本篇供你互動的貓狗辨識應用便是基於此所建立的。

![cnn-architecture](./img/cnn-architecture.jpg)

---

## 2.捲積

---

### 2.1 捲積核 -kernel 

定義:  提取特徵的矩陣, 一般為2維矩陣, 又稱filter

![filter1](./img/filter1.png)

---

### 2.1 捲積核 -kernel 

![filter2](./img/filter2.png)

![filter3](./img/filter3.png)

---

### 2.1 捲積核 -kernel 

除了視覺上的娛樂效果以外，每個濾波器對輸入圖片做的卷積運算事實上是一種[特徵提取（feature extraction）](https://zh.wikipedia.org/wiki/特徵提取)步驟。

不同的濾波器會對圖中所有相同範圍（3 x 3）的像素做不同的轉換，進而從這些像素裡頭提取出：

- 物件輪廓
- 左上突出
- [邊緣線條](https://zh.wikipedia.org/wiki/边缘检测)

---

### 2.2 捲積運算

卷積就是將濾波器裡頭的每個數字拿去跟圖片對應位置的像素值相乘，再把所有相乘結果加起來

![5.1_correlation](./img/5.1_correlation.svg)

![conoulation_math1_Simple](./img/conoulation_math1_Simple.png)

---

### 2.2 捲積運算

![convoulation](./img/convoulation-1604469540603.gif)

---

### 2.2 捲積運算 -edge_dection-code

http://www.gunniliang.com/notebooks/Delete/git_r/two_month_report/202011_2021_1/11_9_to_11_13_second/code/edge_detection.ipynb

----

### 2.2 捲積運算-係數更新

隨機初始化所有濾波器的數值，並利用平常訓練神經網路的反向傳播算法（Backpropagation），讓 CNN 自己學出一組有用的濾波器數值來將輸入圖片（一大堆像素）轉換成我們想要的值（ex:一個貓咪機率）。

![backpropagation-example](./img/backpropagation-example.gif)

---

### 3.池化

目的:降低取樣, 減低運算複雜度

Average-pooling: 取均值

Max-pooling: 取最大值

![max-pooling-steps](./img/max-pooling-steps.jpg)

---

### 3.池化-過程

![max-pooling-a](./img/max-pooling-a.png)



---

### 3.池化--效果

直觀來說，一個 2 x 2 的池化運算能讓我們在不影響物體的情況下，將原圖大小縮減到原來的 25 %。

![downsampling](./img/downsampling.jpg)

---

### 3.池化 -Example

http://www.gunniliang.com/notebooks/Delete/git_r/two_month_report/202011_2021_1/11_9_to_11_13_second/code/pooling_ex.ipynb





---

### 捲積網路概略整理:

- CNN 是一個利用卷積與池化對輸入圖片做特徵擷取的神經網路架構
- 對圖片做卷積是有意義的數據轉換是因為：                                    
  - 很多圖形 pattern 尺寸比原圖小很多
  - 同樣 pattern 會重複出現在很多地方
- 對圖片做池化是有意義的數據轉換是因為：                                    
  - 對像素降低取樣並不會改變圖中物件
  - 減少神經網路所需處理的數據量
- 卷積後常跟著池化運算，而你可以重複做（卷積 -> 池化）步驟多次來萃取圖片特徵
- 最後得到的圖片特徵可以交給 CNN 裡的全連接層，由它為我們做（貓狗）分類
- CNN 裡的卷積、池化扮演著萃取特徵的角色，而全連接層則扮演著分類器的角色

---

### 4.Alexnet 模型

2012年，AlexNet橫空出世。這個模型的名字來源於論文第一作者的姓名Alex  Krizhevsky。AlexNet使用了8層卷積神經網絡，並以很大的優勢贏得了ImageNet  2012圖像識別挑戰賽。

它首次證明了學習到的特征可以超越手工設計的特征，從而一舉打破計算機視覺研究的前狀。

AlexNet 的架構共八層，第一層到第五層是 Convolutional Layers 做卷積操作（Convolution Operation）與池化（Pooling）；第六層到第八層是 Fully Connected Layers。

---

### 4.Alexnet 模型

![alexnet_arch2](./img/alexnet_arch2.jpg)

---

### 4.Alexnet 模型-激活函數

Activation Function：使用 ReLU
 當時最常用的激活函數（Activation Function）是 Sigmoid 和 tanh function。Sigmoid 是把輸入的值壓縮到 0 和 1 之間，但它有`梯度消失`的缺點會讓神經網絡很難被優化。

![alexnet_activation](./img/alexnet_activation.png)



---

### 4.Alexnet 模型-降低 Overfitting 的方法

- Dropout
   在 AlexNet 中，第六層和第七層的全連階層使用 Dropout，配置為 0.5，表示每個神經元有 50% 的機率不參與下一層的傳遞。這種技術迫使神經網絡需要學習更為穩健的特徵，因此可有效降低 Overfitting。
- Data augmentation
   AlexNet 的處理分為兩種方式：
  1. 原始圖片的像素是 256 * 256，進行隨機抽取其中的 224 * 224 ，且允許水平翻轉將資料擴增 2048 倍。論文中說明此作法能有效的避免 Overfitting。
  2. 改變RGB通道的強度：對 RGB 色彩空間做主成份分析（PCA），接著用高斯隨機擾動。這個方法是透過自然圖片的性質來實現，也就是該物體對於照明的強度和顏色的變化是不變的。透過這個方法，top-1 的錯誤率下降 1%。



---

### 4.Alexnet 模型-其他特點

大數據訓練
 百萬數量級的 ImageNet 圖像數據所訓練出來的模型。

GPU實現
 AlexNet 在訓練時用了兩塊 GTX 580 GPU，由於內存 3GB 仍不足以處理如此大量的資料，因此神經網絡分成上下兩層，一塊 GPU 處理一部分的數據，並在中間部分層做溝通。

---

### 4.Alexnet 模型-code



### 補充 Dropout, BathNormalization,Adam

---

ref

https://ithelp.ithome.com.tw/articles/10205088

